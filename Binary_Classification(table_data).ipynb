{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcb2f05-458b-48c9-8772-e4dcbb8e920c",
   "metadata": {},
   "source": [
    "# Binary_Classification(table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35334f3d-a8a2-48d8-ae07-4071e2cdb259",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c538ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) matplotlib 설정\n",
    "plt.rc(\"font\", family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "plt.rc('figure', figsize=(8, 4))\n",
    "plt.rc('figure', dpi=100)\n",
    "\n",
    "# 2) 이렇게 seaborn scheme을 세팅해도 됨\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "import missningno as msno\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d642784",
   "metadata": {},
   "source": [
    "# 해야할 프로세스\n",
    "1. 데이터셋 확인 - null data 확인, 수정\n",
    "2. 탐색적 데이터 분석(Exploratory data analysis - 여러 feature 개별분석, 상관관계 확인, 시각화 툴로 insight\n",
    "3. feature engineering - OneHot Encoding, Class로 나누기, 구간으로 나누기, 텍스트 데이터 처리 등\n",
    "4. model\n",
    "5. fit & predict\n",
    "6. test - 풀려는 문제에 따라 모델 평가방식이 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35271a0",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6078b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    NROWS = 50000\n",
    "else:\n",
    "    NROWS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ada4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터가 csv인 경우\n",
    "df_train = pd.read_csv('', nrows=NROWS)\n",
    "df_test = pd.read_csv('', nrows=NROWS)\n",
    "\n",
    "# 또는\n",
    "import os\n",
    "path = '경로/'\n",
    "df_train = pd.read_csv(os.path.join(path, '파일명'), nrows=NROWS)\n",
    "df_test = pd.read_csv(os.path.join(path, '파일명'), nrows=NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터가 너무 많으면 샘플링해서 EDA하기\n",
    "데이터 = 데이터.sample(frac=0.1)\n",
    "\n",
    "# imbalanced data가 걱정이면\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "fold = StratifiedKFold(n_splits=10)\n",
    "for trn_idx, val_idx in fold.split(데이터, 데이터['타겟']):\n",
    "    break\n",
    "데이터 = 데이터.iloc[trn_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceea549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비트 수로 데이터 용량 줄이기\n",
    "for c in train.select_dtypes(include=['float64']).columns:\n",
    "    train[c]=train[c].astype(np.float32)\n",
    "    test[c]=test[c].astype(np.float32)\n",
    "for c in train.select_dtypes(include=['int64']).columns[2:]:\n",
    "    train[c]=train[c].astype(np.int8)\n",
    "    test[c]=test[c].astype(np.int8)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e10b4e",
   "metadata": {},
   "source": [
    "# 1. Dataset 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ab16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련, 테스트 데이터의 칼럼이 같은지 확인\n",
    "set(trainset.columns) - set(testset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터.describe() # describe()는 숫자형 데이터만 보여준다\n",
    "\n",
    "# 특정 열을 지정해서 describe() 하면 count와 unique값을 계산해 줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1853a62",
   "metadata": {},
   "source": [
    "- null data가 존재하는 열(feature)을 확인\n",
    "- 변수의 자료형 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터.info() # info()는 모든 데이터를 보여주고 데이터 타입도 알 수 있다(dtypes로도 확인 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터.drop_duplicates() # 중복된 행이 있으면 안되는 경우 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fda31",
   "metadata": {},
   "source": [
    "## 1.1 메타 데이터\n",
    "- **role**: input, ID, target\n",
    "- **level**: binary, categorical, ordinal, interval\n",
    "- **keep**: True or False\n",
    "- **dtype**: int, float, str\n",
    "\n",
    "**porto 데이터셋에만 해당하는 코드이므로 차후에 일반화된 코드로 바꿀 것**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708564bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for f in train.columns:\n",
    "    # Defining the role\n",
    "    if f == '타겟변수':\n",
    "        role = 'target'\n",
    "    elif f == '아이디':\n",
    "        role = 'id'\n",
    "    else:\n",
    "        role = 'input'\n",
    "        \n",
    "    # Defining the level\n",
    "    if 'bin' in f or f == '타겟변수':\n",
    "        level = 'binary'\n",
    "    elif 'cat' in f or f == '아이디':\n",
    "        level = 'categorical'\n",
    "    elif train[f].dtype == float:\n",
    "        level = 'interval'\n",
    "    elif train[f].dtype == 'int64':   # int32, 64 등 확인하고 설정할 것\n",
    "        level = 'ordinal'\n",
    "        \n",
    "    # Initialize keep to True for all variables except for id\n",
    "    keep = True\n",
    "    if f == '아이디':\n",
    "        keep = False\n",
    "    \n",
    "    # Defining the data type\n",
    "    dtype = train[f].dtype\n",
    "    \n",
    "    category = 'none'\n",
    "    # 칼럼 이름에 따른 어떤 분류가 필요한 경우\n",
    "    if 'ind' in feature:\n",
    "        category = 'individual'\n",
    "    elif 'reg' in feature:\n",
    "        category = 'registration'\n",
    "    elif 'car' in feature:\n",
    "        category = 'car'\n",
    "    elif 'calc' in feature:\n",
    "        category = 'calculated'\n",
    "    \n",
    "    # Creating a Dict that contains all the metadata for the variable\n",
    "    f_dict = {\n",
    "        'varname': f,\n",
    "        'role': role,\n",
    "        'level': level,\n",
    "        'keep': keep,\n",
    "        'dtype': dtype,\n",
    "        'category': category\n",
    "    }\n",
    "    data.append(f_dict)\n",
    "    \n",
    "meta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype', 'category'])\n",
    "meta.set_index('varname', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9ef9c",
   "metadata": {},
   "source": [
    "- meta의 변수 level 잘 할당되었는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf388a",
   "metadata": {},
   "source": [
    "#### 메타데이터 확인 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[(meta.level == 'categorical') & (meta.keep)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'count': meta.groupby(['role', 'level'])['role'].size()}).reset_index() # size includes NaN values, count does not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a93ce",
   "metadata": {},
   "source": [
    "## 1.2 Null data check\n",
    "- isnull().sum(), msno, bar 등으로 null값 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ee7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isnull()로 시각화\n",
    "# 훈련 데이터, 테스트 데이터 확인할 것\n",
    "vars_with_missing = []\n",
    "\n",
    "for f in 데이터.columns:\n",
    "    missings = 데이터[f].isnul().sum()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings / 데이터.shape[0]\n",
    "        \n",
    "        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n",
    "        \n",
    "print('In total, there are {} variables with missing values'.format(len(vars_with_missing)))\n",
    "\n",
    "\n",
    "\n",
    "# Null값이 NaN이 아닌 다른 값으로 (-1) 표시되어 있을 때\n",
    "vars_with_missing = []\n",
    "\n",
    "for f in 데이터.columns:\n",
    "    missings = 데이터[데이터[f] == -1][f].count()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings / 데이터.shape[0]\n",
    "        \n",
    "        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n",
    "        \n",
    "print('In total, there are {} variables with missing values'.format(len(vars_with_missing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1685140c",
   "metadata": {},
   "source": [
    "- 데이터에 존재하는 null 값이 퍼센트로 표시된다\n",
    "- 그냥 데이터.isnull().sum()으로 해도됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d94e7",
   "metadata": {},
   "source": [
    "#### MSNO matrix로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023173c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터, 테스트 데이터 확인할 것\n",
    "\n",
    "msno.matrix(df=데이터.iloc[:, :], figsize=(8, 8), color=(0.8, 0.5, 0.2)) \n",
    "\n",
    "# color는 RGB\n",
    "# 굳이 .iloc 안 붙여도 됨\n",
    "# 이 차트는 null값의 분포를 보기 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac545c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# msno의 bar로 시각화\n",
    "# 훈련 데이터, 테스트 데이터 확인할 것\n",
    "\n",
    "msno.bar(df=데이터.iloc[:, :], figsize=(8, 8), color=(0.8, 0.5, 0.2))\n",
    "\n",
    "# 이 차트는 null값의 비율을 보기 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576fd1c",
   "metadata": {},
   "source": [
    "## 1.3 Target label 확인\n",
    "- target label의 분포를 확인\n",
    "\n",
    "- binary classification 문제의 경우 1과 0의 분포에 따라 모델 평가 방법이 달라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입을 확인(카테고리형, 연속형, 순서형, 이진형, 문자열 등등)\n",
    "데이터['타겟'].value_counts()  # 칼럼을 두개 지정하면?? 그 데이터로 bar() 하면??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e77930",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Pie plot\n",
    "데이터['타겟'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True) # explode 는 범주값 개수만큼 지정\n",
    "ax[0].set_title('Pie plot - 타겟')\n",
    "ax[0].set_ylabel('')\n",
    "# Count plot\n",
    "sns.countplot('타겟', data=데이터, ax=ax[1])\n",
    "ax[1].set_title('Count plot - 타겟')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23731db8",
   "metadata": {},
   "source": [
    "- Target label의 분포가 균일한지 확인해야 한다. 분포가 극단적으로 치우쳐져 있다면 accuracy만으로 성능 측정 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601fb5e5",
   "metadata": {},
   "source": [
    "### 1.3.1 Target label이 불균형할 때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1c077",
   "metadata": {},
   "source": [
    "### 1.3.1.1 undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7de7f4",
   "metadata": {},
   "source": [
    "#### Random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35df5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "desired_apriori = 0.10\n",
    "\n",
    "# Get the indices per target value\n",
    "idx_0 = 데이터[데이터['타겟'] == 0].index\n",
    "idx_1 = 데이터[데이터['타겟'] == 1].index\n",
    "\n",
    "# Get original number of records per target value\n",
    "nb_0 = len(데이터.loc[idx_0])\n",
    "nb_1 = len(데이터.loc[idx_1])\n",
    "\n",
    "# Calculatet the undersampling rate and resulting number of records with target=0\n",
    "undersampling_rate = ((1 - desired_apriori) * nb_1) / (desired_apriori * nb_0)\n",
    "undersampled_nb_0 = int(undersampling_rate * nb_0)\n",
    "print('Rate to undersample records with target=0: {}'.format(undersampling_rate))\n",
    "print('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n",
    "\n",
    "# Randomly select records with target=0 to get at the desired a priori\n",
    "undersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n",
    "\n",
    "# Construct list with remaining indices\n",
    "idx_list = list(undersampled_idx) + list(idx_1)\n",
    "\n",
    "# Return undersample data frame\n",
    "데이터 = 데이터.loc[idx_list].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee37a5c",
   "metadata": {},
   "source": [
    "## 1.4 Outlier 확인\n",
    "- 연속형 feature에서 사용\n",
    "- null값이 있는 칼럼은 np.percentile로 계산 안되므로 isnull() or notnull()을 이용해 null값 제외하고 계산\n",
    "- train데이터에서만 수행해야 한다. valid, test에서 하면 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82a550-b40d-4124-941f-69ee2e1838e9",
   "metadata": {},
   "source": [
    "### 1.4.1 아웃라이어를 2칼럼 이상 포함하고 있는 행을 지우는 방법\n",
    "- IQR로 아웃라이어 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fafd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Outlier detection\n",
    "# 아웃라이어가 n개 이상 있는 행 조사\n",
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according to the Tukey method.\n",
    "    \"\"\"\n",
    "    \n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[df[col].isnull() == False][col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[df[col].isnull() == False][col], 75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "    \n",
    "    # select obervations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)           # 시퀀스 안의 각 자료들이 몇개인지 세어주는 함수(딕셔너리 반환)\n",
    "    multiple_outliers = list(k for k,v in outlier_indices.items() if v > n)\n",
    "    \n",
    "    return multiple_outliers\n",
    "\n",
    "outliers_to_drop = detect_outliers(데이터, n개, ['변수1', '변수2', '변수3'])\n",
    "데이터.loc[outliers_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd56ea-dabc-4a99-8efa-86f5014fed0c",
   "metadata": {},
   "source": [
    "### 1.4.2 각 feature마다 아웃라이어를 체크하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28641c74-a1ad-4550-803c-c0880cdf0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR로 아웃라이어 체크\n",
    "Q1 = np.percentile(데이터[데이터['변수'].isnull() == False]['변수'], 25)\n",
    "Q3 = np.percentile(데이터[데이터['변수'].isnull() == False]['변수'], 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outlier_step = 1.5 * IQR\n",
    "outlier_indices = (데이터['변수'] < Q1 - outlier_step) | (데이터['변수'] > Q3 + outlier_step)\n",
    "\n",
    "outlier = 데이터[outlier_indices]\n",
    "non_outlier = 데이터[~outlier_indices]\n",
    "\n",
    "# 히스토그램 시각화를 통해 체크\n",
    "sns.distplot(데이터['변수'], bins=, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3300678-9fa5-4d26-ada7-ccb2ed2b2a52",
   "metadata": {},
   "source": [
    "- Home Credit competition `DAYS_EMPLOYED`변수\n",
    "    - 극단값이 꽤 많아서 히스토그램을 그려보면 마치 정상값, 이상값의 binary 변수처럼 보이는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06569d56-4b59-4407-84e8-0055ea077ba8",
   "metadata": {},
   "source": [
    "#### 아웃라이어 데이터와 정상 데이터의 Target 비율 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587aa54-108e-4a5b-bcf4-ba07ff611e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier['타겟'].mean()\n",
    "\n",
    "non_outlier['타겟'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ce1e9-1241-4f01-a353-986258f9d618",
   "metadata": {},
   "source": [
    "- 아웃라이어와 정상 데이터의 Target 비율이 의미있는 차이를 보인다면 삭제하지 않고 이용할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281c590",
   "metadata": {},
   "source": [
    "#### 아웃라이어 drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터 = 데이터.drop(outliers_to_drop, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0722c0",
   "metadata": {},
   "source": [
    "## 1.5 train, test 데이터의 feature 분포 확인\n",
    "- 각 feature들이 train, test 에서 모두 같은 분포를 이루고 있지 않으면 서로 다른 데이터임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(행, 열, figsize=(12, 4))\n",
    "i = 0\n",
    "for feature in 데이터:\n",
    "    i = i + 1\n",
    "    plt.subplot(행, 열, i)\n",
    "    sns.kdeplot(훈련[feature], label='train')\n",
    "    sns.kdeplot(테스트[feature], label='test')\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.ylabel('Distribution', fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "#     plt.setp(labels, rotation=90)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d72bf9",
   "metadata": {},
   "source": [
    "# 2. Exploratory data analysis\n",
    "- 데이터 안에 숨겨진 사실을 찾기 위해 적절한 시각화 필요\n",
    "- 시각화 라이브러리는 matmplotlib, seaborn, plotly 등. 목적에 맞게 소스코드를 정리해 둘 것\n",
    "- 시각화 할 때 눈금 조정으로 인한 실제 값과 라벨의 미스매치 항상 조심할 것\n",
    "- 관측값이 아닌 비율값을 시각화 할 때는 항상 신뢰구간이 보이는 차트를 사용할 것.(데이터가 적은 경우 문제가 됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c50852",
   "metadata": {},
   "source": [
    "## 2.1 일변수 분석\n",
    "- 변수의 값에 따라서 Target label값이 어떻게 다른지를 groupby 또는 pivot으로 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c1caa",
   "metadata": {},
   "source": [
    "### 2.1.1 카테고리형, 이진형 데이터 (서수형도 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 데이터\n",
    "binary = meta[(meta.level == 'binary') & (meta.keep)].index\n",
    "데이터[binary].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91afe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 데이터\n",
    "category = meta[(meta.level == 'categorical') & (meta.keep)].index\n",
    "데이터[category].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfdb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서수형 데이터\n",
    "ordinal = meta[(meta.level == 'ordinal') & (meta.keep)].index\n",
    "데이터[ordinal].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e975f6",
   "metadata": {},
   "source": [
    "#### 모든 변수에 대해서 변수 자체의 분포 확인\n",
    "범주 값이 많은 카테고리형 변수는 한 plot에서 확인 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af588cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary 변수\n",
    "bin_col = [col for col in binary if col != '타겟']\n",
    "zero_list = []\n",
    "one_list = []\n",
    "for col in bin_col:\n",
    "    zero_list.append((데이터[col]==0).sum() / 데이터.shape[0] * 100)\n",
    "    one_list.append((데이터[col]==1).sum() / 데이터.shape[0] * 100)\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "p1 = sns.barplot(x=bin_col, y=zero_list, color='blue', ax=ax)\n",
    "p2 = sns.barplot(x=bin_col, y=one_list, bottom=zero_list, color='red', ax=ax)\n",
    "plt.xlabel('Binary features', fontsize=12)\n",
    "plt.ylabel('Percent of zero/one [%]', font_size=12)\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.legend((p1, p2), ('Zero', 'One'))\n",
    "plt.show()\n",
    "\n",
    "# ordinal\n",
    "\n",
    "# categorical 변수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79939fd",
   "metadata": {},
   "source": [
    "- 만약 어떤 변수가 모두 0 또는 1인 분포라면 의미 없는 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4746b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 모든 변수에 대해서 Target 비율 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045364f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary 변수\n",
    "bin_col = [col for col in binary if col != '타겟']\n",
    "i = 0\n",
    "t1 = 데이터.loc[데이터['타겟'] != 0]\n",
    "t0 = 데이터.loc[데이터['타겟'] == 0]\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(6, 3, figsize=(12, 24))\n",
    "\n",
    "for feature in bin_col:\n",
    "    i += 1\n",
    "    plt.subplot(행, 열, i)\n",
    "    sns.barplot(feature, '타겟', data=데이터, palette='Set2')\n",
    "#   빠르게 하려면 비율을 직접 구해서 바로 넣을수도 있음\n",
    "    plt.ylabel('Density plot', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()\n",
    "\n",
    "# category 변수\n",
    "category = meta[(meta.level == 'categorical') & (meta.keep)].index\n",
    "\n",
    "for feature in category:\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    # Calculate the percentage of target=1 per category value\n",
    "    cat_perc = 데이터[[feature, '타겟']].groupby([feature], as_index=False).mean()\n",
    "    sns.barplot(x=feature, y='타겟', data=cat_perc, order=cat_perc[feature], ax=ax)\n",
    "    plt.ylabel('Percent of target with value 1 [%]', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1107f07",
   "metadata": {},
   "source": [
    "- 시각화 했을 때 신뢰구간이 너무 크면 데이터가 부족하므로 해당 데이터의 신뢰성에 문제가 있을 수 있음\n",
    "    - 비율이 0이거나 1인 데이터 조심(신뢰구간 표시 안됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece142b",
   "metadata": {},
   "source": [
    "#### 모든 변수에 대해서 target 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53692a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary 변수\n",
    "bin_col = [col for col in binary if col != '타겟']\n",
    "i = 0\n",
    "t1 = 데이터.loc[데이터['타겟'] != 0]\n",
    "t2 = 데이터.loc[데이터['타겟'] == 0]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(행, 열, figsize=(6, 6))\n",
    "\n",
    "for feature in bin_col:\n",
    "    i += 1\n",
    "    plt.subplot(행, 열, i)\n",
    "    sns.kdeplot(t1[feature], label='target = 1')\n",
    "    sns.kdeplot(t0[feature], label='target = 0')\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.ylabel('Density plot', fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()\n",
    "\n",
    "# category 변수\n",
    "category = meta[(meta.level == 'categorical') & (meta.keep)].index\n",
    "i = 0\n",
    "t1 = 데이터.loc[데이터['타겟'] != 0]\n",
    "t0 = 데이터.loc[데이터['타겟'] == 0]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(행, 열, figsize=16, 16)\n",
    "for feature in category:\n",
    "    i += 1\n",
    "    plt.subplot(행, 열, i)\n",
    "    sns.kdeplot(t1[feature], label='target = 1')\n",
    "    sns.kdeplot(t0[feature], label='target = 0')\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.ylabel('Density plot', fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfee589",
   "metadata": {},
   "source": [
    "#### 변수 하나씩 process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04fa9e1",
   "metadata": {},
   "source": [
    "#### 변수의 각 범주마다 target 데이터의 전체 관찰값의 균일한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터[['변수', '타겟']].groupby(['변수'], as_index=True).count()  # 총 데이터 개수. value_counts()와 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095a700",
   "metadata": {},
   "source": [
    "#### target 데이터 범주도 균일한지 확인(타겟이 카테고리인 경우도 활용 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(데이터['변수'], 데이터['타겟'], margins=True).style.background_gradient(cmap='summer_r')\n",
    "# 색상 google에 color scheme 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac877f",
   "metadata": {},
   "source": [
    "- 변수 값에 따라 Target값이 어떻게 다른지 확인하고 기록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739ac09",
   "metadata": {},
   "source": [
    "#### 변수의 분포 \n",
    "#### 변수의 각 범주에 따른 target값이 1인 비율 시각화\n",
    "#### countplot으로 변수에 따른 Target 쪼개서 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_position = 1.02\n",
    "f, ax = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# 변수의 분포\n",
    "sns.countplot('변수', data=데이터, ax=ax[0])  # countplot은 bar()와 달리 색깔을 다 다르게 입혀줌\n",
    "# 데이터['변수'].value_counts().plot.bar(color=['#CD7F32', '#FFDF00', '#D3D3D3'], ax=ax[0]) x축이 값이 옆으로 출력됨\n",
    "ax[0].set_title('Number of 타겟 By 변수', y=y_position)\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "# 변수에 따른 Target label이 1인 값 비율 시각화\n",
    "sns.barplot('변수', '타겟', data=데이터, palette='Set2', order=, ax=ax[1]) # 색깔이 다 다름, barplot도 비율을 계산\n",
    "# 데이터[['변수', '타겟']].groupby(['변수'], as_index=True).mean().plot.bar(ax=ax[1]) 로 시각화하면 신뢰구간을 볼 수 없다!!\n",
    "# factorplot으로도 가능\n",
    "ax[1].set_title('타겟 ratio by 변수')\n",
    "\n",
    "# Target 분포 시각화(Target 쪼갬)\n",
    "# t1 = 데이터.loc[데이터['타겟'] != 0]\n",
    "# t0 = 데이터.loc[데이터['타겟'] == 0]\n",
    "# sns.kdeplot(t1['변수'], ax=ax[2])\n",
    "# sns.kdeplot(t0['변수'], ax=ax[2])\n",
    "sns.countplot('변수', hue='타겟', data=데이터, ax=ax[2])\n",
    "# countplot에서 hue에 어떤 칼럼을 지정하면 그 칼럼의 범주에 따라 bar 차트를 쪼갠다 \n",
    "\n",
    "ax[2].set_title('변수: a vs b', y=y_position)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# xlabel 순서 달라져서 각 plot의 x축 순서가 바뀌면 sort_values(ascending=)또는 sort_index()으로 바꿔주기(혹은 order로 순서 바꿀 수 있음)\n",
    "# 눈금 바꿀 때 실제 값과 미스매치 날 수 있으니 항상 조심!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432edcd1",
   "metadata": {},
   "source": [
    "- 시각화 했을 때 신뢰구간이 너무 크면 데이터가 부족하므로 해당 데이터의 신뢰성에 문제가 있을 수 있음\n",
    "    - 비율이 0이거나 1인 데이터 조심(신뢰구간 표시 안됨)\n",
    "    - 해당 데이터를 사용하지 않는 방법도 고려해야 한다\n",
    "    - barplot에 신뢰구간을 표시하려면 ('칼럼', data=dataframe) 형식으로 전달해야 함.  (x, y) 식으로 넣으면 신뢰구간을 구할 수 없다\n",
    "    \n",
    "- category 데이터에서는 이러한 문제를 mean encoding이나 category encoding 등으로 해결한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248edb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구체적인 수치(비율)\n",
    "데이터[['변수', '타겟']].groupby(['변수'], as_index=True).mean()\n",
    "\n",
    "# 타겟이 1인 값들의 총합(타겟 레이블이 0, 1 인 경우만 사용)\n",
    "데이터[['변수', '타겟']].groupby(['변수'], as_index=True).sum()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8fc9a",
   "metadata": {},
   "source": [
    "- 구체적인 수치로 보고 이 변수가 Target을 예측하는데 도움이 되는지 안되는지 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690062bb",
   "metadata": {},
   "source": [
    "#### 카테고리 혹은 서수형 데이터를 이진화할 필요가 있는 경우\n",
    "- ex) Dietanic : FamilySize가 1 이냐 or 2 이상이냐 로 이진화해서 새로운 시각으로 볼 수 있다\n",
    "- 새로운 Feature를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33aed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 변수에서 특정 범주에 해당하느냐 안하느냐로 이진화한 새 변수 생성\n",
    "데이터['새 변수'] = 0\n",
    "데이터.loc[데이터['기존 변수'] == '이진화 범주', '새 변수'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e81da",
   "metadata": {},
   "source": [
    "### 2.1.2 연속형 실수 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5c974",
   "metadata": {},
   "source": [
    "#### 모든 연속형 실수 데이터를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61272eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = meta[(meta.level == 'interval') & (meta.keep)].index\n",
    "train[v].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "t1 = 데이터.loc[데이터['타겟'] != 0]\n",
    "t0 = 데이터.loc[데이터['타겟'] == 0]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(행, 열, figsize=(16, 12))\n",
    "\n",
    "for feature in v:\n",
    "    i += 1\n",
    "    plt.subplot(행, 열, i)\n",
    "    sns.kdeplot(t1[feature], bw=, label='target = 1')\n",
    "    sns.kdeplot(t0[feature], bw=, label='target = 0')\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.ylabel('Density plot', fontsize=12)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea98eb",
   "metadata": {},
   "source": [
    "- 한꺼번에 시각화 해서 그 중 target값이 0이냐 1이냐에 따라 다른 분포를 보이는 변수들을 찾아서 시각화 하는게 효율적\n",
    "- 혹은 target과의 corr만 확인해서 그중 관계성이 높은 것만 탐색하는 방법도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8858be",
   "metadata": {},
   "source": [
    "#### 기술 통계 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터['변수'].max()\n",
    "데이터['변수'].min()\n",
    "데이터['변수'].mean()\n",
    "데이터['변수'].median()\n",
    "\n",
    "# 이걸로 한방에\n",
    "데이터['변수'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c4f09-bf24-48d6-ae85-6f95101036fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# 변수 자체의 분포\n",
    "sns.distplot(데이터['변수'], color='b', bins=, kde=, label='Skewness : {:.2f}'.format(데이터['변수'].skew()), ax=ax[0])\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "# 연속형 변수를 구간화(binning) 해서 Target의 비율을 확인(시각화)\n",
    "bin_interval = 간격\n",
    "range_target_ratio = []\n",
    "sem_by_range = []     # sem : standard error of the mean : 표준오차\n",
    "for i in range(변수최솟값, 변수최댓값, bin_interval):\n",
    "    # 구간별 타겟의 비율 p\n",
    "    p_by_range = 데이터[(데이터['변수'] >= i) & (데이터['변수'] < i + bin_interval)]['타겟'].mean() # 합산값을 구하고 싶다면 sum()\n",
    "    range_target_ratio.append(p_by_range)\n",
    "    # 표준오차 구하기\n",
    "    n_by_range = 데이터[(데이터['변수'] >= i) & (데이터['변수'] < i + bin_interval)]['타겟'].count()\n",
    "    sem_by_range.append(np.sqrt(p_by_range * (1 - p_by_range)) / np.sqrt(n_by_range))\n",
    "\n",
    "sns.barplot(np.arange(변수최솟값, 변수최댓값, bin_interval), range_target_ratio, ax=ax[1], yerr=np.array(sem_by_range)*1.96) # 95%신뢰구간\n",
    "ax[1].set_title('타겟 rate change depending on rage of 변수', y=1.02)\n",
    "ax[1].set_ylabel('타겟 rate')\n",
    "ax[1].set_xlabel('Range of 변수(0~x)')\n",
    "ax[1].set_xticks(np.arange(bin_interval))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 연속형 변수를 구간화(binning) 해서 Target의 비율을 확인(단순 수치 확인)\n",
    "변수_data = 데이터[['타겟', '변수']]\n",
    "변수_data['변수_BINNED'] = pd.cut(데이터['변수'], bins=np.linspace(최소, 최대, 분할))\n",
    "변수_groups = 변수_data.groupby('변수_BINNED').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48920b55",
   "metadata": {},
   "source": [
    "#### 변수 자체의 분포, 변수 구간화한 후 Target의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd162bc",
   "metadata": {},
   "source": [
    "- 변수 자체의 분포 에서\n",
    "    - 변수가 정규분포인지 균일분포인지, 불균형 데이터는 아닌지 파악\n",
    "    - null값 처리를 아직 하지 않았으므로 불균형 분포라고 지금 log변환하면 안된다!!!!!!\n",
    "        - feature engineering에서 null처리, log변환 후 다시 돌아와서 EDA\n",
    "        - null값이 없으면 log취해도 됨\n",
    "    - 연속형 변수의 분포를 확인할 때 주의점\n",
    "        - 이상치 값으로 인해 시각화에 왜곡이 생길경우 이상치를 제외한 분포도 확인해 바야 함(Home Credit Comepetition의 DAYS_EMPLOYED 변수)\n",
    "    \n",
    "- 연속형 변수를 구간화(binning) 후 Target의 비율에서\n",
    "    - 구간은 나누는 방법이 매우 중요\n",
    "    - 이렇게 시각화 하면 신뢰구간 안 나옴\n",
    "    - 신뢰구간이 나오지 않으므로 왼쪽 플롯의 실제 관측값의 개수와 비교해 볼 것\n",
    "    - 신뢰구간 표시하려면 표준편차 구해서 xerr혹은yerr 인자로 전달(0 이나 1같은 극단적인 값은 신뢰구간 못구하니까 조심)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b7d4b",
   "metadata": {},
   "source": [
    "#### 변수값에 따른 Target의 kde(커널밀도추정) 또는 히스토그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5485f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(9, 5))\n",
    "sns.kdeplot(데이터[데이터['타겟'] == 0]['변수'], ax=ax)\n",
    "sns.kdeplot(데이터[데이터['타겟'] == 1]['변수'], ax=ax)\n",
    "plt.legend(['타겟 == 0', '타겟 == 1'])\n",
    "plt.show()\n",
    "\n",
    "# 데이터[데이터['타겟'] == 1]['변수'].plot(kind='kde')\n",
    "# 데이터[데이터['타겟'] == 0]['변수'].plot(kind='kde') 로도 표현가능\n",
    "\n",
    "# 정확한 값을 보고싶다면 히스토그램을 그려야 하는데\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "sns.distplot(데이터[데이터['변수'] == 1]['타겟'], bins=, kde=, ax=ax[0]) # 이렇게 하면 kde까지 볼 수 있음\n",
    "# 데이터[데이터['타겟'] == 1]['변수'].plot.hist(ax=ax[0], color='b', bins=, edgecolor='k')  # plot.hist() : plot(kind='hist')\n",
    "ax[0].set_title('타겟 == 1')\n",
    "ax[0].set_xlim([ , ])\n",
    "ax[0].set_xticks(np.arange( , , ))\n",
    "\n",
    "sns.distplot(데이터[데이터['변수'] == 0]['타겟'], bins=, kde=, ax=ax[0])\n",
    "# 데이터[데이터['타겟'] == 0]['변수'].plot.hist(ax=ax[1], color='b', bins=, edgecolor='k')\n",
    "ax[1].set_title('타겟 == 0')\n",
    "ax[1].set_xlim([ , ])\n",
    "ax[1].set_xticks(np.arange( , , ))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f8a57",
   "metadata": {},
   "source": [
    "- kde의 경우 비율 값 아님, 관측값 기준\n",
    "- Target값이 0일 때의 변수의 그래프, Target값이 1일 때의 변수의 그래프를 비교했을 때 차이를 보면 \n",
    "- 연속형 변수의 값에 따른 Target label의 분포를 알 수 있다\n",
    "- 연속형 변수의 분포를 확인할 때 주의점\n",
    "    - 이상치 값으로 인해 시각화에 왜곡이 생길경우 이상치를 제외한 분포도 확인해 바야 함(Home Credit Comepetition의 DAYS_EMPLOYED 변수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e6210",
   "metadata": {},
   "source": [
    "#### 변수2(연속형)를 구간화 하지 않고 누적시켜 가면서 타겟의 변화를 시각화 하려면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulate_변수_ratio = []\n",
    "\n",
    "for i in range(변수최솟값 , 변수최댓값):\n",
    "    cummulate_변수_ratio.append(데이터[데이터['변수'] < i]['타겟'].sum() / len(데이터[데이터['변수'] < i]['타겟']))\n",
    "    \n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(cummulate_변수_ratio)\n",
    "plt.title('타겟 rate change depending on range of 변수')\n",
    "plt.ylabel('타겟 rate')\n",
    "plt.xlabel('Range of 변수(0~x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12da95c",
   "metadata": {},
   "source": [
    "## 2.2 이변수 분석\n",
    "- 한 변수에 따라 다른 한 변수의 분포 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06778d3a",
   "metadata": {},
   "source": [
    "### 2.2.1 두 변수 모두 카테고리형 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(데이터['변수1'], 데이터['변수2'], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033f4f7",
   "metadata": {},
   "source": [
    "- 두 변수 모두 순서형 데이터인 경우에는 상관계수로 heatmap을 그려볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140fc02f",
   "metadata": {},
   "source": [
    "#### 두 변수의 분포를 관찰횟수로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.countplot('변수1', hue='변수2', data=데이터)\n",
    "\n",
    "# sns.factorplot('변수1', col='변수2', kind='count', data=데이터) 로도 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b59bab",
   "metadata": {},
   "source": [
    "#### 두 변수의 분포를 비율로 시각화 - pie plot\n",
    "- 변수 1의 각 범주에서 변수2의 분포가 어떻게 되는지를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie plot\n",
    "f, ax = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "train[train['변수1'] == '범주1']['변수2'].value_counts().sort_index().plot.pie(explode=[0, 0, 0], autopct='%1.1f%%', shadow=True, ax=ax[0])\n",
    "ax[0].set_title('변수2 in 범주1')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "train[train['변수1'] == '범주2']['변수2'].value_counts().sort_index().plot.pie(explode=[0, 0, 0], autopct='%1.1f%%', shadow=True, ax=ax[1])\n",
    "ax[1].set_title('변수2 in 범주2')\n",
    "ax[1].legend(loc='upper right')\n",
    "\n",
    "train[train['변수1'] == '범주3']['변수2'].value_counts().sort_index().plot.pie(explode=[0, 0, 0], autopct='%1.1f%%', shadow=True, ax=ax[2])\n",
    "ax[2].set_title('변수2 in 범주2')\n",
    "ax[2].legend(loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 라벨 색상이 모두 같게 해야 하는데 실수의 여지가 있으므로 반드시 세 차트 모두 라벨 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9eec3c",
   "metadata": {},
   "source": [
    "- 변수1과 타겟에 대한 일변수 분석을 할때 의문이 이변수 분석에서 나올 수 있다.\n",
    "- Titanic의 EDA to Dietanic Notebook에서 SibSp가 높을수록 생존률이 높다가 3명 이상부터 떨어지는데 그 원인이 SibSp가 3 이상인 가족은 모두 Pclass가 3이었음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2b934",
   "metadata": {},
   "source": [
    "### 2.2.2 변수1: 카테고리, 변수2: 연속형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5c7c7",
   "metadata": {},
   "source": [
    "#### kdeplot(한 figure에 모두 표시)\n",
    "#### kdeplot은 비율로 표시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "데이터['변수2(연속형)'][데이터['변수1(카테고리형)'] == 범주1].plot(kind='kde') \n",
    "데이터['변수2(연속형)'][데이터['변수1(카테고리형)'] == 범주2].plot(kind='kde')\n",
    "데이터['변수2(연속형)'][데이터['변수1(카테고리형)'] == 범주3].plot(kind='kde')\n",
    "\n",
    "plt.xlabel('변수2(연속형)')\n",
    "plt.title('변수2 Distribution within classes')\n",
    "plt.legend(['1st Class', '2nd Class', '3rd Class', ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e146c0",
   "metadata": {},
   "source": [
    "- 이 방식이 violinplot보다 가독성이 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a07e5f",
   "metadata": {},
   "source": [
    "#### 각각 나눠서 표시\n",
    "#### histogram은 관측값으로 표시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "데이터['변수2(연속형)'][데이터['변수1(카테고리형)'] == 범주1].plot(kind='hist', ax=ax[0], xlim=[ , ]) # x축 스케일 통일\n",
    "데이터['변수2(연속형)'][데이터['변수1(카테고리형)'] == 범주2].plot(kind='hist', ax=ax[1], xlim=[ , ])\n",
    "데이터['변수2(연속형)'][데이터['변수1(카테고리형)'] == 범주3].plot(kind='hist', ax=ax[2], xlim=[ , ])\n",
    "...\n",
    "plt.xlabel('변수2(연속형)')\n",
    "plt.title('변수2 Distribution within classes')\n",
    "plt.legend(['1st Class', '2nd Class', '3rd Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e3e2a",
   "metadata": {},
   "source": [
    "#### distplot은 histogram을 비율로 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4dbb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "sns.distplot(데이터[데이터['변수1(카테고리형)'] == 범주1]['변수2(연속형)'], ax=ax[0])\n",
    "ax[0].set_title('변수2 in 변수1의 범주1')\n",
    "ax[0].set_xlim([ , ])\n",
    "\n",
    "sns.distplot(데이터[데이터['변수1(카테고리형)'] == 범주2]['변수2(연속형)'], ax=ax[1])\n",
    "ax[1].set_title('변수2 in 변수1의 범주2')\n",
    "ax[1].set_xlim([ , ])\n",
    "\n",
    "sns.distplot(데이터[데이터['변수1(카테고리형)'] == 범주3]['변수2(연속형)'], ax=ax[2])\n",
    "ax[2].set_title('변수2 in 변수1의 범주3')\n",
    "ax[2].set_xlim([ , ])\n",
    "\n",
    "# x축 스케일 통일해야 왜곡 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46473e3b",
   "metadata": {},
   "source": [
    "- 변수1의 각 범주에 따라 변수2의 분포를 살펴본다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc6e0a",
   "metadata": {},
   "source": [
    "### 2.2.3 두 변수 모두 연속형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74f131",
   "metadata": {},
   "source": [
    "#### 모든 연속형 변수에 대해 상관계수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66350879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(v):\n",
    "    correlations = 데이터[v].corr()\n",
    "    \n",
    "    # Create color map ranging between two colors\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f', square=True, linewidths=0.5, annot=True,\n",
    "                cbar_kws={'shrinks': 0.75})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = meta[(meta.level == 'interval') & (meta.keep)].index\n",
    "corr_heatmap(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ba79f",
   "metadata": {},
   "source": [
    "- heatmap상 correlation이 나타나지 않는다고 두 변수가 상관성이 없다고 단정할 수 없음\n",
    "- 두 변수 모두 순서형 데이터인 경우에도 그려볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007e43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7abfed78",
   "metadata": {},
   "source": [
    "#### 상호정보량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53493c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c953493",
   "metadata": {},
   "source": [
    "### 2.2.4 변수1에 대해 다른 모든 변수로 이변수 분석\n",
    "- 일변수 분석으로 하나씩 늘려갈 때 마다 이전 분석한 일변수들과 이변수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(다른 모든 변수 갯수만큼, figsize=(15, 10))\n",
    "\n",
    "# 변수1(카테고리형) 분포\n",
    "sns.countplot('변수1', data=데이터, ax=ax[0, 0])\n",
    "ax[0, 0].set_title('(1) No. Of 변수1')\n",
    "\n",
    "# 변수2(카테고리형)로 split\n",
    "sns.countplot('변수1', hue='변수2', data=데이터, ax=ax[ , ])\n",
    "ax[ , ].set_title('(2) 변수2 split for 변수1')\n",
    "\n",
    "# 변수3(연속형)으로 split\n",
    "데이터['변수3(연속형)'][데이터['변수1(카테고리형)'] == 범주1].plot(kind='kde', ax=ax[ , ])\n",
    "데이터['변수3(연속형)'][데이터['변수1(카테고리형)'] == 범주2].plot(kind='kde', ax=ax[ , ])\n",
    "데이터['변수3(연속형)'][데이터['변수1(카테고리형)'] == 범주3].plot(kind='kde', ax=ax[ , ])\n",
    "...\n",
    "ax[ , ].xlabel('변수3(연속형)')\n",
    "ax[ , ].title('변수3 Distribution within classes')\n",
    "ax[ , ].legend('1st Class', '2nd Class', '3rd Class', ...)\n",
    "\n",
    "ax[ , ].set_title('(3) 변수 vs 타겟')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ae74b",
   "metadata": {},
   "source": [
    "## 2.3 이변수 - Target 분석\n",
    "- 두 변수에 관하여 Target이 어떻게 다른지를 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6def1",
   "metadata": {},
   "source": [
    "### 2.3.1 두 변수 모두 카테고리형, 서수형 변수\n",
    "- 변수1의 각 범주에서 변수2의 범주에 따라 Target이 어떻게 달라지는지 확인\n",
    "- 변수2의 각 범주에서 변수1의 범주에 따라 Target이 어떻게 달라지는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7752d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = meta[(meta.level == 'binary') & (meta.keep)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047aa38",
   "metadata": {},
   "source": [
    "#### 꺾은선 그래프로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x축에 변수1, y축에 타겟으로 하고 변수2의 그래프를 추가\n",
    "sns.factorplot('변수1', '타겟', hue또는col='변수2', data=데이터, size=5, aspect=1.5)\n",
    "\n",
    "# x축에 변수2, y축에 타겟으로 하고 변수1의 그래프를 추가\n",
    "sns.factorplot('변수2', '타겟', hue또는col='변수1', data=데이터, size=5, aspect=1.5)\n",
    "\n",
    "# hue는 합쳐서, col은 나눠서, hue와 col을 동시에 사용할 수도 있음\n",
    "# kind='bar'로 하면 그래프가 아니나 막대형태 표현 가능\n",
    "\n",
    "# catplot or factorplot 은 figure 수준의 plot이므로 subplot을 듣지 않음\n",
    "# factorplot은 없어짐, catplot 알아볼 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651aa52",
   "metadata": {},
   "source": [
    "#### crosstab으로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([데이터['변수1'], data['타겟']], data['변수2'], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22110b",
   "metadata": {},
   "source": [
    "#### pairplot으로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 데이터.sample(frac=0.05)\n",
    "var = ['변수1', '변수2', '타겟']\n",
    "sample = sample[var]\n",
    "sns.pairplot(sample, hue='타겟', palette='Set1', diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d2b65",
   "metadata": {},
   "source": [
    "### 2.3.2 변수1: 카테고리, 변수2: 연속형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed9f16",
   "metadata": {},
   "source": [
    "#### 변수2(연속형) 값에 따른 Target kde를 변수1(카테고리형)의 각 범주마다 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(30, 8))\n",
    "\n",
    "# 변수1(카테고리) 범주 1\n",
    "sns.kdeplot(데이터[(데이터['타겟'] == 0) & (데이터['변수1(카테고리형)'] == 범주1)]['변수2(연속형)'], ax=ax[0])\n",
    "sns.kdeplot(데이터[(데이터['타겟'] == 1) & (데이터['변수1(카테고리형)'] == 범주1)]['변수2(연속형)'], ax=ax[0])\n",
    "ax[0].legend(['타겟 == 0', '타겟 == 1'])\n",
    "ax[0].set_title('변수1의 범주1')\n",
    "\n",
    "# 변수1(카테고리) 범주 2\n",
    "sns.kdeplot(데이터[(데이터['타겟'] == 0) & (데이터['변수1(카테고리형)'] == 범주2)]['변수2(연속형)'], ax=ax[1])\n",
    "sns.kdeplot(데이터[(데이터['타겟'] == 1) & (데이터['변수1(카테고리형)'] == 범주2)]['변수2(연속형)'], ax=ax[1])\n",
    "ax[1].legend(['타겟 == 0', '타겟 == 1'])\n",
    "ax[1].set_title('변수1의 범주2')\n",
    "\n",
    "# 변수1(카테고리) 범주 3\n",
    "sns.kdeplot(데이터[(데이터['타겟'] == 0) & (데이터['변수1(카테고리형)'] == 범주3)]['변수2(연속형)'], ax=ax[2])\n",
    "sns.kdeplot(데이터[(데이터['타겟'] == 1) & (데이터['변수1(카테고리형)'] == 범주3)]['변수2(연속형)'], ax=ax[2])\n",
    "ax[2].legend(['타겟 == 0', '타겟 == 1'])\n",
    "ax[2].set_title('변수1의 범주3')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 커널 함수식으로 추정한 그래프이며 정확한 히스토그램은\n",
    "데이터['변수2(연속형)'][(데이터['변수1(카테고리형)'] == 1) & (데이터['타겟'] == 0)].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221bfbc",
   "metadata": {},
   "source": [
    "- 변수2(연속형)에 따른 Target의 분포를 변수1의 각 범주마다 분리해서 시각화\n",
    "- 이 방식이 violinplot보다 가독성이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a95338",
   "metadata": {},
   "source": [
    "#### 연속형 변수를 y축으로 놓고 violinplot 시각화를 하면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "sns.violinplot('변수1(카테고리형)', '변수2(연속형)', hue='타겟', data=데이터, scale='count', split=True, ax=ax)\n",
    "ax.set_title('변수1 and 변수2 vs 타겟')\n",
    "ax.set_yticks(range(최솟값, 최댓값, 간격))\n",
    "plt.show()\n",
    "\n",
    "# scale='area'는 면적을 같게 해서 distribution 차이를 보기 쉽게,\n",
    "# scale='count'는 실제 관찰값의 개수를 알아보기 쉽게\n",
    "# scale='width'는?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76223f64",
   "metadata": {},
   "source": [
    "#### 연속형 변수를 카테고리화 한 후 factorplot으로도 시각화 가능(아직 categorize안 했으므로 plot으로 구현)\n",
    "- 때로는 이 방법이 변수관계를 더욱 효과적으로 보여준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ccef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나눌 구간 확인\n",
    "데이터['변수_range'] = pd.qcut(data['변수'], 구간개수) \n",
    "데이터.groupby(['변수_range'])['타겟'].mean().to_frame().style.background_gradient(cmap='summer_r')\n",
    "\n",
    "# groupby후 mean()을 하면 Series가 되기때문에 style을 쓸수 없음 => to_frame()으로 DataFrame화\n",
    "\n",
    "# qcut은 샘플수를 최대한 맞춰주면서 분할\n",
    "# cut은 그냥 간격을 똑같이 맞춤(한 쪽 class로 데이터가 몰릴 수도 있다\n",
    "# 둘 다 해보고 value_counts()로 믿을만 한지 확인해봐야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.subplots( , , 1)\n",
    "변수1_범주1 = []\n",
    "for i in range(최솟값, 최댓값, 간격):\n",
    "    변수1_범주1.append(데이터[(데이터['변수2'] >= i) & (데이터['변수2'] < i + 간격) & (데이터['변수1'] == 범주1)]['타겟'].mean())\n",
    "plt.plot(변수1_범주1)\n",
    "plt.title('타겟 rate in 변수1 범주1')\n",
    "plt.ylabel('타겟 rate')\n",
    "plt.xlabel('변수2')\n",
    "plt.xticks(np.arange(구간))\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplots( , , 2)\n",
    "변수1_범주2 = []\n",
    "for i in range(최솟값, 최댓값, 간격):\n",
    "    변수1_범주1.append(데이터[(데이터['변수2'] >= i) & (데이터['변수2'] < i + 간격) & (데이터['변수1'] == 범주2)]['타겟'].mean())\n",
    "plt.plot(변수1_범주2)\n",
    "plt.title('타겟 rate in 변수1 범주2')\n",
    "plt.xlabel('변수2')\n",
    "plt.xticks(np.arange(구간))\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplots( , , 3)\n",
    "변수1_범주3 = []\n",
    "for i in range(최솟값, 최댓값, 간격):\n",
    "    변수1_범주1.append(데이터[(데이터['변수2'] >= i) & (데이터['변수2'] < i + 간격) & (데이터['변수1'] == 범주3)]['타겟'].mean())\n",
    "plt.plot(변수1_범주3)\n",
    "plt.title('타겟 rate in 변수1 범주3')\n",
    "plt.xlabel('변수2')\n",
    "plt.xticks(np.arange(구간))\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "    \n",
    "# 간격을 정하는 것이 매우 중요\n",
    "# y축 스케일 일치시켜야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4cd68c",
   "metadata": {},
   "source": [
    "#### pairplot으로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 데이터.sample(frac=0.05)\n",
    "var = ['변수1', '변수2', '타겟']\n",
    "sample = sample[var]\n",
    "sns.pairplot(sample, hue='타겟', palette='Set1', diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987463f",
   "metadata": {},
   "source": [
    "### 2.3.3 두 변수 모두 연속형인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = meta[(meta.level == 'interval') & (meta.keep)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d51a9",
   "metadata": {},
   "source": [
    "#### 회귀직선 그리기\n",
    "- 상관관계가 나타나는 변수들을 직접 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = train.sample(frac=0.1)  # 데이터 개수가 너무 많으면 표본 일부만 추출해서 그리기\n",
    "\n",
    "sns.lmplot(x='변수1', y='변수2', data=데이터, hue='타겟', palette='Set1', scatter_kws={'alpha':0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62e1d4",
   "metadata": {},
   "source": [
    "- alpha를 0.3으로 했으므로 색이 진하면 값이 많아서 겹쳤다는 의미\n",
    "- target값 0과 1의 회귀직선이 다르면 변수간 상호작용 효과가 있다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ed87b",
   "metadata": {},
   "source": [
    "#### pairplot으로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edece45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 데이터.sample(frac=0.05)\n",
    "var = ['변수1', '변수2', '타겟'] # 모든 연속 형 변수를 넣어서 그릴 수도 있음\n",
    "sample = sample[var]\n",
    "sns.pairplot(sample, hue='타겟', palette='Set1', diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d97433",
   "metadata": {},
   "source": [
    "- target이 1인 것과 0인것의 분포 차이가 나타나는 변수조합을 찾아낼 수 잇음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551be88",
   "metadata": {},
   "source": [
    "#### kde plot으로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(trainset[trainset['target'] != 0]['ps_reg_01'], bw=, label='target = 1')\n",
    "sns.kdeplot(trainset[trainset['target'] == 0]['ps_reg_01'], bw=, label='target = 0')\n",
    "plt.xlim([, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc7c85",
   "metadata": {},
   "source": [
    "- bw: 커널 크기를 지정하는 듯?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f9159",
   "metadata": {},
   "source": [
    "## 2.4 세 개의 변수 - Target 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddf07b",
   "metadata": {},
   "source": [
    "### 2.4.1 변수가 모두 카테고리형 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4c9a4",
   "metadata": {},
   "source": [
    "#### crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([데이터['변수1'], 데이터['변수2']], \n",
    "            [데이터['변수3'], 데이터['타겟']], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d42f8",
   "metadata": {},
   "source": [
    "- 변수1, 변수2는 x축, 변수3, 타겟은 y축\n",
    "- 타겟의 분포를 각각의 변수로 모두 split해서 살펴볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0c8f6",
   "metadata": {},
   "source": [
    "#### factorplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('변수1', '타겟', hue='변수2', col='변수3', size=5, aspect=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98830ec2",
   "metadata": {},
   "source": [
    "# EDA시 주의사항\n",
    "\n",
    "- 1) 연속형 변수를 구간화해서 x축으로 둔 시각화를 할 때 연속형 변수가 정규분포면 각 구간이 균등하지 않음 \n",
    "      => 양쪽 끝 구간은 표본이 크지 않다 => 이상한 값이 나올 가능성이 크다\n",
    "      - 이런 현상은 카테고리값에서 각 범주가 균등하지 않을때도 발생, 데이터가 적은 범주에서 도출된 결과는 신뢰성이 낮음\n",
    "- 2) log변환 등 처리를 null값 처리 후에 해야 하므로 Feature Engineering단계에서 log변환 후 해당 변수에 대해 EDA단계로 돌아와야 함\n",
    "\n",
    "#### 데이터에서 무언가 찾아낼 수 있는 것, 또는 사례\n",
    "- porto-seguro-exploratory-analysis-and-prediction 노트북에서\n",
    "    - 'ps_car_15', 'ps_car_15' 칼럼이 원래 값에서 10으로 나눈 후 제곱근을 씌운 값이라는 것을 알아냄\n",
    "- 데이터 값이 0 ~ 1사이 값이면 어떤 비율을 의미하는 것일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bf6d7",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "- **train, valid, test중 어디에 적용해야할 지 주의**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b0b71e",
   "metadata": {},
   "source": [
    "## 3.1 Null값 처리\n",
    "- Null값이 존재하는 칼럼이 몇개인지 다시 확인\n",
    "- 대부분의 Feature engineering 전에 해야 함 ex) log\n",
    "- train데이터에서의 추측값으로 valid, test를 채워야 한다(valid는 나중에 valid 데이터로 채울수 있을듯)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70509fe2",
   "metadata": {},
   "source": [
    "### 3.1.1 사용하지 않음\n",
    "- Null값이 너무 많은 칼럼은 정보가 별로 없으므로 나중에 drop()\n",
    "- meta데이터에서 keep=False로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary, nominal, ordinal 변수의 경우\n",
    "\n",
    "데이터[['변수', '타겟']].groupby('변수').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1fc076",
   "metadata": {},
   "source": [
    "- NaN 값 자체가 정보가 될 수도 있으니 체크해 봐야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터.drop('변수', inplace=True, axis=1)\n",
    "meta.loc[(vars_to_drop), 'keep'] = False # Updating the meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e7b94",
   "metadata": {},
   "source": [
    "### 3.1.2 단순 통계량으로 처리\n",
    "- Null값 개수가 몇 개 안되거나 특정값이 매우 많은 경우 가장 많은 값으로 치환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b326c",
   "metadata": {},
   "source": [
    "#### 카테고리 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d632a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 방법, 가장 많은 값으로 치환\n",
    "훈련['변수'].fillna('가장많은 값', inplace=True)\n",
    "테스트['변수'].fillna('가장많은 값', inplace=True)\n",
    "\n",
    "# sklearn 이용한 방법\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "어떤방법_imp = SimpleImputer(missing_values=nan, strategy='most_frequent')\n",
    "훈련['변수'] = 어떤방법_imp.fit_transform(훈련[['변수']]).ravel()\n",
    "테스트['변수'] = 어떤방법_imp.transform(테스트[['변수']]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ef28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null값이 채워졌는지 확인\n",
    "데이터['변수'].isnull().sum()  # 또는 any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e891a",
   "metadata": {},
   "source": [
    "#### 연속형 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중위값으로 채우는 경우\n",
    "데이터['변수'] = 데이터['변수'].fillna(데이터['변수'].median())\n",
    "\n",
    "# 평균으로 채우는 경우\n",
    "데이터['변수'] = 데이터['변수'].fillna(데이터['변수'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bcea6e",
   "metadata": {},
   "source": [
    "### 3.1.3 다른 Feature의 정보로부터 추측할 수 있는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b87cd",
   "metadata": {},
   "source": [
    "### 3.1.3.1 채울 Feature: 연속형, 추측 Feature: 카테고리형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d095c6",
   "metadata": {},
   "source": [
    "#### boxplot으로 Feature들과 분포 연관성 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a761df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "f, ax = plt.subplots(1, 2, figsize=(20, 12))\n",
    "\n",
    "sns.boxplot(y='채울변수', x='변수1', data=데이터, ax=ax[0, 0])\n",
    "sns.boxplot(y='채울변수', x='변수1', hue='변수2', data=데이터, ax=ax[0, 1])\n",
    "\n",
    "# hue=변수2(카테고리형)을 추가하면 변수1를 또다른 변수2로 쪼갤 수 있음\n",
    "\n",
    "# sns.factorplot(y='채울변수', x='변수1', col='변수2', data=데이터, kind='box') 변수2로 쪼갠 범주를 각 figure로 분산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98739538",
   "metadata": {},
   "source": [
    "- boxplot보다 kdeplot을 겹치는게 더 가독성이 좋음(이변수-target분석-연속형, 카테고리형 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 개의 이상의 Feature를 이용한 mean 혹은 median 계산할 수도 있다\n",
    "데이터.groupby(['추정변수1', '추정변수2'])['채울변수'].mean()\n",
    "\n",
    "데이터.groupby(['추정변수1', '추정변수2'])['채울변수'].count() # 데이터 개수가 신뢰성 있을만큼 많은지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03708dc",
   "metadata": {},
   "source": [
    "- 두 개 이상의 Feature를 이용해 null값을 채울 수도 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2a2c0",
   "metadata": {},
   "source": [
    "#### 상관계수로 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e09687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관계수\n",
    "sns.heatmap(dataset[['Age', 'Sex', 'SibSp', 'Parch', 'Pclass']].corr(), cmap='BrBG', annot=True)\n",
    "\n",
    "# 카테고리형 변수의 string 값을 numerical 값으로 변환해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9e0e9",
   "metadata": {},
   "source": [
    "#### 또 다른 방법들\n",
    "\n",
    "- 타이타닉 튜토리얼 : 이름의 Mr., Mrs., Master, Ms., Miss, master, Dr. 등 에서 나이를 추측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a12250",
   "metadata": {},
   "source": [
    "#### Null 값 채우는 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfeb236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for문 방법, 다른 feature의 대푯값으로 채우고 그것도 NaN이면 채울변수 대푯값으로 채움\n",
    "\n",
    "index_NaN_채울변수 = list(데이터['채울변수'][dataset['채울변수'].isnull()].index) # 채울변수 null값 인덱스 리스트\n",
    "\n",
    "채울변수_med = 데이터['채울변수'].median() 혹은 mean()\n",
    "for i in index_NaN_age:\n",
    "    채울변수_pred = 데이터['채울변수'][((데이터['추정변수1'] == 데이터.iloc[i]['추정변수1']) & \n",
    "                                       (데이터['추정변수2'] == 데이터.iloc[i]['추정변수2']) & \n",
    "                                       (데이터['추정변수3'] == 데이터.iloc[i]['추정변수3']))].median() 혹은 mean()\n",
    "    if not np.isnan(채울변수_pred):             # pandas는 .isnull()  numpy는 .isnan()\n",
    "        데이터['채울변수'].iloc[i] = 채울변수_pred\n",
    "    else:\n",
    "        데이터['채울변수'].iloc[i] = 채울변수_med\n",
    "        \n",
    "\n",
    "\n",
    "# 하드코딩 방법\n",
    "# train\n",
    "데이터_train.loc[(데이터_train['채울변수'].isnull()) & (데이터_train['추정변수1'] == '범주1') & (데이터_train['추정변수2'] == '범주1'), '채울변수'] = \n",
    "데이터_train.loc[(데이터_train['채울변수'].isnull()) & (데이터_train['추정변수1'] == '범주1') & (데이터_train['추정변수2'] == '범주2'), '채울변수'] = \n",
    "데이터_train.loc[(데이터_train['채울변수'].isnull()) & (데이터_train['추정변수1'] == '범주1') & (데이터_train['추정변수2'] == '범주3'), '채울변수'] = \n",
    "\n",
    "데이터_train.loc[(데이터_train['채울변수'].isnull()) & (데이터_train['추정변수1'] == '범주2') & (데이터_train['추정변수2'] == '범주1'), '채울변수'] = \n",
    "데이터_train.loc[(데이터_train['채울변수'].isnull()) & (데이터_train['추정변수1'] == '범주2') & (데이터_train['추정변수2'] == '범주2'), '채울변수'] = \n",
    "데이터_train.loc[(데이터_train['채울변수'].isnull()) & (데이터_train['추정변수1'] == '범주2') & (데이터_train['추정변수2'] == '범주3'), '채울변수'] = \n",
    "\n",
    "데이터_train.loc[(데이터_train['채울변수'].isnull()) & (데이터_train['추정변수1'] == '범주3') & (데이터_train['추정변수2'] == '범주1'), '채울변수'] = \n",
    "데이터_train.loc[(데이터_train['채울변수'].isnull()) & (데이터_train['추정변수1'] == '범주3') & (데이터_train['추정변수2'] == '범주2'), '채울변수'] = \n",
    "데이터_train.loc[(데이터_train['채울변수'].isnull()) & (데이터_train['추정변수1'] == '범주3') & (데이터_train['추정변수2'] == '범주3'), '채울변수'] = \n",
    "\n",
    "# train에서의 통계값으로 test를 채움\n",
    "valid혹은test.loc[(valid혹은test['채울변수'].isnull()) & (valid혹은test['추정변수1'] == '범주1') & (valid혹은test['추정변수2'] == '범주1'), '채울변수'] = \n",
    "valid혹은test.loc[(valid혹은test['채울변수'].isnull()) & (valid혹은test['추정변수1'] == '범주1') & (valid혹은test['추정변수2'] == '범주2'), '채울변수'] = \n",
    "valid혹은test.loc[(valid혹은test['채울변수'].isnull()) & (valid혹은test['추정변수1'] == '범주1') & (valid혹은test['추정변수2'] == '범주3'), '채울변수'] = \n",
    "\n",
    "valid혹은test.loc[(valid혹은test['채울변수'].isnull()) & (valid혹은test['추정변수1'] == '범주2') & (valid혹은test['추정변수2'] == '범주1'), '채울변수'] = \n",
    "valid혹은test.loc[(valid혹은test['채울변수'].isnull()) & (valid혹은test['추정변수1'] == '범주2') & (valid혹은test['추정변수2'] == '범주2'), '채울변수'] = \n",
    "valid혹은test.loc[(valid혹은test['채울변수'].isnull()) & (valid혹은test['추정변수1'] == '범주2') & (valid혹은test['추정변수2'] == '범주3'), '채울변수'] = \n",
    "\n",
    "데이터_valid혹은test.loc[(valid혹은test['채울변수'].isnull()) & (valid혹은test['추정변수1'] == '범주3') & (valid혹은test['추정변수2'] == '범주1'), '채울변수'] = \n",
    "데이터_valid혹은test.loc[(valid혹은test['채울변수'].isnull()) & (valid혹은test['추정변수1'] == '범주3') & (valid혹은test['추정변수2'] == '범주2'), '채울변수'] = \n",
    "데이터_valid혹은test.loc[(valid혹은test['채울변수'].isnull()) & (valid혹은test['추정변수1'] == '범주3') & (valid혹은test['추정변수2'] == '범주3'), '채울변수'] = \n",
    "\n",
    "\n",
    "\n",
    "# null값 채워졌는지 확인\n",
    "데이터_train['Age'].isnull().sum()\n",
    "데이터_test['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803c883",
   "metadata": {},
   "source": [
    "## 3.2 log정규분포인 경우 log취할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fceb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log 취하기전에 null값을 모두 채워야 한다. log취한 후 null값 채우면 스케일이 달라짐\n",
    "\n",
    "데이터['변수'] = 데이터['변수'].map(lambda i: np.log(i) i > 0 else 0)\n",
    "# train, valid, test에 모두 적용\n",
    "\n",
    "# 정규분포가 되었는지 확인\n",
    "# 실수로 log를 여러번 취할 수 있으므로 시각화 코드를 동시에 실행하여 실수 방지\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "sns.displot(데이터['변수'], color='b', label='Skewness : {:.2f}'.format(데이터['변수'].skew()), ax=ax)\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a25859",
   "metadata": {},
   "source": [
    "## 3.3 Continuous 데이터의 Categorize\n",
    "- 자칫 정보 손실이 일어날 수 있으니 주의해서 사용\n",
    "- train에서 만든 범주로 valid, test에 모두 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c4076",
   "metadata": {},
   "source": [
    "#### 구간 개수에 따른 구간 범위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d16460",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터['변수_range'] = pd.qcut(data['변수'], 구간개수) # qcut는 샘플수를 최대한 일정하게 맞춰서 나눠줌\n",
    "데이터.groupby(['변수_range'])['타겟'].mean().to_frame().style.background_gradient(cmap='summer_r')\n",
    "# groupby후 mean()을 하면 Series가 되기때문에 style을 쓸수 없음 => to_frame()으로 DataFrame화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af49e5",
   "metadata": {},
   "source": [
    "#### 하드코딩 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터['변수_cat'] = 0\n",
    "데이터.loc[데이터['변수'] <= 첫 구간, '변수'] = 0\n",
    "데이터.loc[(데이터['변수'] <= 구간) & (데이터['변수'] > 구간), '변수_cat'] = 1\n",
    "데이터.loc[(데이터['변수'] <= 구간) & (데이터['변수'] > 구간), '변수_cat'] = 2\n",
    "데이터.loc[(데이터['변수'] <= 구간) & (데이터['변수'] > 구간), '변수_cat'] = 3\n",
    "...\n",
    "데이터.loc[(데이터['변수'] > 마지막 구간), '변수_cat'] = 9\n",
    "# train, valid, test에 모두 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7588c2",
   "metadata": {},
   "source": [
    "#### 함수를 이용한 apply 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3cfa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터['변수_cat'] = 0\n",
    "\n",
    "def category_변수(x):\n",
    "    if x < 10:\n",
    "        return 0\n",
    "    elif x < 20:\n",
    "        return 1\n",
    "    ...\n",
    "    else:\n",
    "        return 9\n",
    "    \n",
    "# train, valid, test에 모두 적용\n",
    "데이터['변수_cat'] = 데이터['변수'].apply(category_변수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f4762",
   "metadata": {},
   "source": [
    "#### pd.cut 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터['변수_binned'] = pd.cut(데이터['변수'], bins=np.linspace(최소, 최대, 분할갯수))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c2d01",
   "metadata": {},
   "source": [
    "#### sklearn.preprocessing KBinsDiscretizer 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "kb = KBinsDiscretizer(n_bins=, strategy='uniform')\n",
    "kb.fit(데이터)\n",
    "\n",
    "X_binned = kb.transform(데이터)  # sparse-matrix를 반환\n",
    "X_binned.toarray() # 원핫인코딩 된 array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ffd4a",
   "metadata": {},
   "source": [
    "#### 원래 변수 칼럼 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터.drop(['변수'], axis=1, inplace=True) # drop()함수는 axis=1 이 칼럼 drop이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade9b94",
   "metadata": {},
   "source": [
    "## 3.4 상호작용 변수\n",
    "- 트리 모델의 경우 상호작용 변수를 자동으로 찾으므로 의미가 없거나 혹은 성능이 더 떨어질 수도 있다\n",
    "- 뉴럴넷을 사용한다면? 은닉층이 상호작용을 자동으로 계산하는 역할을 할 듯?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "poly.fit(훈련['변수1', '변수2'])\n",
    "X_poly_train = poly.transform(훈련['변수1', '변수2'])\n",
    "X_poly_test = poly.transform(테스트['변수1', '변수2'])\n",
    "# transform은 numpy를 반환하므로 dataframe으로 만들어야 함\n",
    "X_poly_train = pd.DataFrame(X_poly_train, columns=poly.get_feature_names(['변수1', '변수2']))\n",
    "X_poly_test = pd.DataFrame(X_poly_test, columns=poly.get_feature_names(['변수1', '변수2']))\n",
    "\n",
    "훈련데이터 = pd.concat([훈련, X_poly_train], axis=1)\n",
    "테스트데이터 = pd.concat([테스트, X_poly_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ac91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 feature의 이름을 얻는 속성\n",
    "poly.get_feature_names(['넣었던 변수들 이름'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f193295",
   "metadata": {},
   "source": [
    "- include_bias : 절편에 해당하는 1인 특성을 추가할 것인가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd299ea",
   "metadata": {},
   "source": [
    "#### 상호작용 다른 예\n",
    "- Home credit 대회 LightGBM 7th place solution 노트북\n",
    "    - 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3' 변수에 min, max, mean, nanmedian, var등의 aggregation 변수 생성\n",
    "    - do_mean, do_median등 여러 feature groupby한 feature 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36269107",
   "metadata": {},
   "source": [
    "## 3.5 Category Feature의 어떤 한 범주값을 binary화\n",
    "- Home Credit competition(start here a gentle introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140af1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터['Is범주'] = 0\n",
    "데이터.loc[데이터['변수(카테고리)'] == 범주, 'Is범주'] = 1\n",
    "\n",
    "# 훈련, 테스트 데이터 모두 바꿔야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae7b90",
   "metadata": {},
   "source": [
    "## 3.6 칼럼에서 정보를 추출하는 사례들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a37599",
   "metadata": {},
   "source": [
    "- Titanic Top 4% with ensemble modeling: Ticket 접두어 처리(string Feature 처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1bfb5",
   "metadata": {},
   "source": [
    "## 3.7 String => Numerical\n",
    "- 값이 문자열인 카테고리 변수를 정수 인코딩\n",
    "- 가능한 나중에 하는 것이 좋음. 미리 해놓으면 범주값 식별이 힘들듯?\n",
    "- train, valid, test에 모두 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e3afc",
   "metadata": {},
   "source": [
    "#### 변수 갯수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터['변수'].value_counts() # series\n",
    "데이터['변수'].unique() # array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply 방법\n",
    "데이터['변수'] = 데이터['변수'].map({'범주0': 0, '범주1': 1, ... 'Other': 9})\n",
    "\n",
    "# replace를 이용한 방법\n",
    "데이터['변수'].replace(['범주1', '범주2', '범주3'], [0, 1, 2], inplace=True)\n",
    "\n",
    "# pandas.factorize   (LabelEncoder보다 2배 가량 빠름)\n",
    "데이터['변수'], uniques = pd.factorize(데이터['변수'])\n",
    "\n",
    "# 한꺼번에 바꾸는 함수\n",
    "def label_encoder(df, categorical_columns=None):\n",
    "    \"\"\"Encode categorical values as integers (0,1,2,3...) with pandas.factorize. \"\"\"\n",
    "    if not categorical_columns:\n",
    "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    for col in categorical_columns:\n",
    "        df[col], uniques = pd.factorize(df[col])\n",
    "    return df, categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51726b7",
   "metadata": {},
   "source": [
    "#### sklearn.LabelEncoder 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfad7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 데이터\n",
    "binary = meta[(meta.level == 'binary') & (meta.keep)].index\n",
    "binary_string = binary[['string변수1', 'string변수2']]\n",
    "\n",
    "for b in binary_string:\n",
    "    le = LabelEcoder()\n",
    "    le.fit(훈련[b])\n",
    "    훈련[b] = le.transform(훈련[b])\n",
    "    테스트[b] = le.transform(테스트[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 데이터\n",
    "category = meta[(meta.level == 'categorical') & (meta.keep)].index\n",
    "category_string = category[['string변수1', 'string변수2']]\n",
    "\n",
    "for c in category_string:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(훈련[c])\n",
    "    훈련[c] = le.transform(훈련[c])\n",
    "    테스트[c] = le.transform(테스트[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36341565",
   "metadata": {},
   "source": [
    "## 3.8 두 칼럼(변수)의 값을 합쳐야 하는 경우\n",
    "- Sibling + Spouse + Parents + Child = Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터['새 변수'] = 데이터['변수1'] + 데이터['변수2']\n",
    "# train, valid, test에 모두 적용\n",
    "\n",
    "# 새로 만든 변수로 EDA를 한다\n",
    "데이터['새변수'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac209f0",
   "metadata": {},
   "source": [
    "## 3.9 변수간의 관계 확인(원-핫인코딩 전에 수행)\n",
    "- string값을 정수인코딩 한 후에 해야 한다\n",
    "- Target을 제외한 변수들 간에 -1 혹은 1인 값들이 존재할 경우 redundant데이터, 필요없다\n",
    "- 다른 변수에서의 정보로 어떤 변수의 null값을 채울 경우 상관관계가 나타날 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = 데이터[['변수1', '변수2', ...]]\n",
    "\n",
    "colormap = plt.cm.RdYlGn\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(heatmap_data.astype(float).corr(), \n",
    "            linewidths=0.1, vmax=1.0, cmap=colormap, linecolor='white', annot=True, annot_kws={'size': 16}, fmt='.2f')\n",
    "\n",
    "# data.corr() --> correlation matrix\n",
    "# linewidths: 칸 간격, vmax: 색깔 최댓값, annot: 수치표시, fmt: 소수점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6ab60-da0e-4d2e-bcb6-2f17ac0db574",
   "metadata": {},
   "source": [
    "- 히트맵이 까맣게 나오는 이유\n",
    "    - 변수가 아주 많은 경우에 annot=True로 주석 글씨 때문\n",
    "- 히트맵이 모두 하얗게 나오는 이유\n",
    "    - 변수가 아주 많은 경우에 linecolor='white', linewidth 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4fa8f-3e8a-4cb3-b5c5-07834feb729a",
   "metadata": {},
   "source": [
    "#### 변수가 너무 많아서 heatmap으로 못 그릴 때는 수치로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869a21f-1bff-477e-8f48-0b1054d2b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correlations with the target and sort\n",
    "correlations_target = 데이터.corr()['타겟'].sort_values()\n",
    "correlations = 데이터.corr()\n",
    "\n",
    "# Display correlations_target\n",
    "print('Most Positive Correlations:\\n', correlations_target.tail(15))\n",
    "print('\\nMost Negative Correlations:\\n', correlations_target.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831fb1a",
   "metadata": {},
   "source": [
    "## 3.10 Categorical 변수 인코딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98e4b1",
   "metadata": {},
   "source": [
    "### 3.10.1 One-hot Encoding\n",
    "- 카테고리(nominal) 데이터만 원핫인코딩, 순서형(Ordinal) 변수는 One-Hot Encoding 하지 않음\n",
    "- from sklearn.preprocessing import OneHotEncoder\n",
    "- 선형 모델에서 사용, 트리 모델에서는 유용하지 않음\n",
    "- high cardinality로 인해 너무 많은 feature가 생길 경우 tree-base model에서 samples a fraction of features로 split시 not one-hot encoded feature에 기회가 적게 돌아간다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터[\"변수\"] = 데이터['변수'].astype('category')\n",
    "데이터 = pd.get_dummies(데이터, columns=['변수'], prefix='원핫인코딩의 접두사', dummy_na=True)\n",
    "\n",
    "# 카테고리 변수들을 한꺼번에 처리(meta는 train으로 만들었으므로 train만 적용되었음, test도 수행)\n",
    "v = meta[(meta.level == 'nominal') & (meta.keep)].index\n",
    "훈련데이터 = pd.get_dummies(훈련데이터, columns=v, drop_first=)\n",
    "\n",
    "# pd.get_dummies()로 원핫인코딩을 할 경우 훈련데이터의 카테고리 변수에는 있는 범주값이 테스트 데이터에터에는 없어서\n",
    "# 테스트 데이터의 해당 범주값의 원핫인코딩 feature가 안생길 수도 있으므로 그런 범주를 삭제\n",
    "# 약간의 정보손실이 생기는 것이므로 주의. \n",
    "# 이렇게 하지 않으려면 scikit-learn의 one-hot encoder 사용\n",
    "훈련_타겟 = 훈련['타겟'] \n",
    "훈련데이터, 테스트데이터 = 훈련데이터.align(테스트데이터, join='inner', axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a811972",
   "metadata": {},
   "source": [
    "- train, valid, test에 모두 적용\n",
    "- get_dummies함수는 원래 칼럼을 삭제한다, 문자열 칼럼에 적용됨, integer 칼럼에는 적용 안됨\n",
    "    - dummy_na: null값에 대한 feature를 만들지 결정\n",
    "- train, valid, test의 원핫인코딩이 같은 값으로 인코딩 되었는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491323b",
   "metadata": {},
   "source": [
    "### 3.10.2 Mean Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb45cf1",
   "metadata": {},
   "source": [
    "- 카테고리 변수의 범주 개수가 너무 많은 경우\n",
    "- 각 범주값들을 해당 범주에서의 타겟 값의 비율로 치환하고 이 변수를 연속형 변수 취급할 수 있음\n",
    "- 오버피팅 가능성이 있음(각 범주에서 데이터 개수가 충분해야 함)\n",
    "    - 베르누이 기댓값은 np > 5 , n(1 - p) > 5 가 만족되어야 정규분포를 따름\n",
    "    - 대부분의 high cardinality feature들은 각 범주에서 sample이 별로 없다는게 문제\n",
    "    - 베르누이 기댓값이 0.5, 때 n이 10인 경우 95% 신뢰구간은 +-0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f919fc",
   "metadata": {},
   "source": [
    "#### 첫 번째 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# porto competitions : xgboost-cv-lb-284\n",
    "# 오버피팅을 막기 위해 노이즈를 넣어줌\n",
    "def add_noise(Series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following by Daniele Micci-Barreca\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    \n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg(['mean', 'count'])\n",
    "    \n",
    "    # Compute Smoothing(use sigmoid function)\n",
    "    smoothing = 1 / (1 + np.exp(-(averages['count'] - min_samples_leaf) / smoothing))\n",
    "    # 각 범주의 관측된 값이 많을수록 smoothing 값이 1에 가까워짐, 적으면 smoothing값이 0에 가까워짐\n",
    "    \n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    \n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages['mean'] * smoothing\n",
    "    averages.drop(['mean', 'count'], axis=1, inplace=True)\n",
    "    \n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}), \n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    \n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    \n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    \n",
    "    # pd.merge does not keep the index so resotre it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    \n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309dd4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 모두 바꿔줘야 한다\n",
    "\n",
    "train_encoded, val_encoded, test_encoded = target_encode(훈련['변수'], \n",
    "                                                         검증['변수'], \n",
    "                                                         테스트['변수'], \n",
    "                                                         min_samples_leaf=100, \n",
    "                                                         smoothing=10, \n",
    "                                                         noise_level=0.01)\n",
    "\n",
    "훈련['변수_te'] = train_encoded\n",
    "훈련.drop('변수', axis=1, inplace=True)\n",
    "meta.loc['변수', 'keep'] = False   # Updating the meta\n",
    "\n",
    "검증['변수_te'] = val_encoded\n",
    "\n",
    "테스트['변수'] = test_encoded\n",
    "테스트.drop('변수', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa01b39",
   "metadata": {},
   "source": [
    "각 범주값마다 target(0, 1)의 가중평균값을 계산하겠다는 것인데\n",
    "\n",
    "관찰값이 많은 범주의 경우 그 범주에서의 target의 mean값에 높은 가중치를 주고\n",
    "\n",
    "관찰값이 적은 범주의 경우 target의 전체 mean값에 높은 가중치를 줘서 가중 평균한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45777325",
   "metadata": {},
   "source": [
    "### Regularized mean encoding\n",
    "- high cardinality인 경우 4 ~ 5 fold, alpha 5를 적용하는게 좋음\n",
    "- mean encoding과 frequency encoding 둘다 쓰는게 좋을 수도 있다\n",
    "- 데이터와 cardinality가 매우 클 경우 expanding도 고려 가능\n",
    "- low cardinality에서는 label or frequency 인코딩을 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encode(train_data, test_data, columns, target_col, reg_method=None,\n",
    "                alpha=0, add_random=False, rmean=0, rstd=0.1, folds=1):\n",
    "    '''Returns a DataFrame with encoded columns'''\n",
    "    encoded_cols = []\n",
    "    target_mean_global = train_data[target_col].mean()\n",
    "    for col in columns:\n",
    "        # Getting means for test data\n",
    "        nrows_cat = train_data.groupby(col)[target_col].count()     # 각 범주에서 샘플 갯수\n",
    "        target_means_cats = train_data.groupby(col)[target_col].mean()    # 각 범주에서 target_mean\n",
    "        target_means_cats_adj = (target_means_cats*nrows_cat + target_mean_global*alpha) / (nrows_cat+alpha)  # 가중 평균\n",
    "        \n",
    "        # Mapping means to test data\n",
    "        encoded_col_test = test_data[col].map(target_means_cats_adj)    # map 함수에 Series를 넣을 수 있음\n",
    "        \n",
    "        # Getting a train encodings\n",
    "        if reg_method == 'expanding_mean':  # 각 범주값 내에서 무작위를 또 추가할 수 있음\n",
    "            train_data_shuffled = train_data.sample(frac=1, random_state=1)\n",
    "            cumsum = train_data_shuffled.groupby(col)[target_col].cumsum() - train_data_shuffled[target_col] # train_data_shuffled.groupby(col)[target_col].cumsum()  각 범주에서 target 누적합을 의미\n",
    "            cumcnt = train_data_shuffled.groupby(col).cumcount() # 각 범주값이 몇번째 중복되는지를 계산 0이면 해당 범주값이 처음 나왔음을 의미\n",
    "            encoded_col_train = cumsum / (cumcnt) # 각 범주에서 target mean값을 누적 계산\n",
    "            encoded_col_train.fillna(target_mean_global, inplace=True)   # 한번도 안 나온 범주값은 target 전체 평균로 대체\n",
    "            if add_random:\n",
    "                encoded_col_train = encoded_col_train + normal(loc=rmean, scale=rstd, size=(encoded_col_train.shape[0]))  # 노이즈 추가\n",
    "        \n",
    "        elif (reg_method == 'k_fold') and (folds > 1):\n",
    "            kfold = StratifiedKFold(train_data[target_col].values, folds, shuffle=True, random_state=1)\n",
    "            parts = []\n",
    "            for tr_in, val_ind in kfold:\n",
    "                # divide data\n",
    "                df_for_estimation, df_estimated = train_data.iloc[tr_in], train_data.iloc[val_ind]\n",
    "                \n",
    "                # getting means on data for estimation (all folds except estimated)\n",
    "                nrows_cat = df_for_estimation.groupby(col)[target_col].count()\n",
    "                target_means_cats = df_for_estimation.groupby(col)[target_col].mean()\n",
    "                target_means_cats_adj = (target_means_cats*nrows_cat + target_mean_global*alpha) / (nrows_cat+alpha)\n",
    "                \n",
    "                # Mapping means to estimated fold\n",
    "                encoded_col_train_part = df_estimated[col].map(target_means_cats_adj)\n",
    "                if add_random:\n",
    "                    encoded_col_train_part = encoded_col_train_part + normal(loc=rmean, scale=rstd, size=(encoded_col_train_part.shape[0]))\n",
    "                \n",
    "                # Saving estimated encodings for a fold\n",
    "                parts.append(encoded_col_train_part)\n",
    "            encoded_col_train = pd.concat(parts, axis=0)\n",
    "            encoded_col_train.fillna(target_mean_global, inplace=True)\n",
    "            \n",
    "        else:\n",
    "            encoded_col_train = train_data[col].map(target_means_cats_adj)\n",
    "            if add_random:\n",
    "                encoded_col_train = encoded_col_train + normal(loc=rmean, scale=rstd, size=(encoded_col_train.shape[0]))\n",
    "    \n",
    "    return incoded_col_train, encoded_col_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded, test_encoded = mean_encode(훈련데이터, 테스트데이터, 카테고리칼럼리스트, 훈련['타겟'], reg_method=None, \n",
    "                                          alpha=0, add_random=False, rmean=0, rstd=0.1, folds=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2042477",
   "metadata": {},
   "source": [
    "- test데이터에는 노이즈 추가나 regularization 하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca2f86",
   "metadata": {},
   "source": [
    "### 3.10.3 Frequency encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d4bc7",
   "metadata": {},
   "source": [
    "#### 한 번에 하나씩\n",
    "- 기존 칼럼을 남겨 놓음\n",
    "- 테스트 데이터 처리 안했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encoding(train_data, test_data, col):\n",
    "    freq_encoding = train_data.groupby([col]).size() / train_data.shape[0]   # size includes NaN values, count does not\n",
    "    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequency'.format(col)})\n",
    "    \n",
    "    return 데이터.merge(freq_encoding, on=col, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25949e7",
   "metadata": {},
   "source": [
    "#### 다른방법(카테고리 칼럼들을 넣어주면 한번에 계산)\n",
    "- 기존 칼럼을 대체\n",
    "- 테스트 데이터도 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fa416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_encode(train_data, test_data, columns):\n",
    "    '''Returns a DataFrame with encoded columns'''\n",
    "    \n",
    "    for col in columns:\n",
    "        freqs_cat = train_data.groupby(col)[col].count() / train_data.shape[0]\n",
    "        encoded_col_train = train_data[col].map(freqs_cat)\n",
    "        encoded_col_test = test_data[col].map(freqs_cat)\n",
    "        encoded_col = pd.concat([encoded_col_train, encoded_col_test], axis=0)\n",
    "        encoded_col[encoded_col.isnull()] = 0\n",
    "    \n",
    "    return encoded_col_train, encoded_col_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70ba54",
   "metadata": {},
   "source": [
    "### 3.10.4 기타 카테고리 변수 인코딩 응용\n",
    "- porto competition 2등 솔루션: 'ind'변수들을 여러개 묶어서 frequency encoding, \n",
    "    - high cardnality 변수가 많을 때 유용\n",
    "    - 거기에 원핫인코딩도 같이 사용해서 변수를 표현\n",
    "- Home Credit 대회 LightGBM 7th place solution노트북\n",
    "    - 여러 변수들 묶어서 do_mean, do_median등으로 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513554fc",
   "metadata": {},
   "source": [
    "## 3.11 특성 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c11c8d",
   "metadata": {},
   "source": [
    "### 3.11.1 분산이 낮은 변수 삭제\n",
    "- 해놓으면 학습속도 높일 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selector.fit(데이터.drop(['단순ID', '타겟'], axis=1))  # Fit to train without id and target variables\n",
    "\n",
    "f = np.vectorize(lambda x : not x)   # Function to toggle boolean array elements\n",
    "v = train.drop(['단순ID', '타겟'], axis=1).columns[f(selector.get_support())] # ~numpy.array(list) 하는 방법도 있다\n",
    "\n",
    "print('{} variables have too low variance.'.format(len(v)))\n",
    "print('These variables are {}'.format(list(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fdfcd",
   "metadata": {},
   "source": [
    "- valid, test에서도 삭제해야 되나?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b694fd",
   "metadata": {},
   "source": [
    "### 3.11.2 일변량 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb64f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd99529e",
   "metadata": {},
   "source": [
    "### 3.11.3 모델 기반 특성 선택\n",
    "- 트리 기반 모델의 feature_importances_\n",
    "- 선형모델의 계수 절댓값, L1 규제 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1035f1",
   "metadata": {},
   "source": [
    "#### RandomForest\n",
    "- 일반적으로 일변량 분석보다 훨씬 강력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863781a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train = 데이터.drop(['단순아이디', '타겟'], axis=1)\n",
    "y_train = 데이터['target']\n",
    "\n",
    "feat_labels = X_train.columns\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f+1, 30, feat_labels[indices[f]], importances[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e14bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sfm = SelectFromModel(rf, threshold='median', prefit=True)\n",
    "print('Number of features before selection: {}'.format(X_train.shape[1]))\n",
    "\n",
    "n_features = sfm.transform(X_train).shape[1]\n",
    "print('Number of features after selection: {}'.format(n_features))\n",
    "\n",
    "selected_vars = list(feat_labels[sfm.get_support()])\n",
    "데이터 = 데이터[selected_vars + ['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e637c9a",
   "metadata": {},
   "source": [
    "- prefit: False로 하면 여기서 새로 다시 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07e060",
   "metadata": {},
   "source": [
    "### 3.11.4 반복적 특성 선택(Recursive feature elimination)\n",
    "- 10개씩 또는 20개씩\n",
    "- 20 모델 baseline\n",
    "- random choosing 20\n",
    "- 40 모델 학습 -> baseline 비교\n",
    "- if 성능 향상 -> feature importance 상위 10%에 새로 추가된 거 생기면\n",
    "- 걔를 남기고\n",
    "- 안생기면\n",
    "- 다른 거 randomly choosing \n",
    "- 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c4e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c54fed50",
   "metadata": {},
   "source": [
    "### 3.11.5 전문가 지식 활용 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763952d9",
   "metadata": {},
   "source": [
    "- Home Credit Competition의 start here a gentle introduction노트북\n",
    "    - CREDIT_INCOME_PERCENT, ANNUITY_INCOME_PERCENT, CREDIT_TERM, DAYS_EMPLOYED_PERCENT 4개의 새로운 feature를 만들어 냄\n",
    "    - LIGHTGBM에서 효과 있었음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d64c9",
   "metadata": {},
   "source": [
    "#### 주의사항\n",
    "- 도메인 지식으로 새로운 feature를 만들었다면 이 feature에 대해서도 EDA를 진행할 것\n",
    "- 그리고 그 새로운 feature가 중요한지 확인하려면 feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f56657",
   "metadata": {},
   "source": [
    "#### 필요 없는 칼럼 drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train, valid, test 모두 적용\n",
    "데이터.drop(['변수1', '변수2', ...], axis=1, inplace=True)\n",
    "\n",
    "# df.drop은 특이하게 axis=1이 칼럼 삭제를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c775311",
   "metadata": {},
   "source": [
    "### 그외 칼럼 선택, 삭제 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd181d9",
   "metadata": {},
   "source": [
    "- 어떤 특정 문자로 시작하는 칼럼 삭제하려면 : porto-seguro-exploratory-analysis-and-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67393c0",
   "metadata": {},
   "source": [
    "## 3.12 스케일링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24e93a",
   "metadata": {},
   "source": [
    "### 3.12.1 Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f3787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(데이터.drop['타겟'], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=( , ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2491e5",
   "metadata": {},
   "source": [
    "# Pairplot 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df54772",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(데이터[['변수1', '변수2', '변수3', '변수4', '변수5']], \n",
    "             hue='Survived', palette='seismic', size=1.2, diag_kind=dict(shade=True), plot_kws=dict(s=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10db7a",
   "metadata": {},
   "source": [
    "# 메모리 사용량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beff4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory(df):\n",
    "    \"\"\"Reduce memory usage of a dataframe by setting data types. \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Initial df memory usage is {:.2f} MB for {} columns'\n",
    "          .format(start_mem, len(df.columns)))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            cmin = df[col].min()\n",
    "            cmax = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # Can use unsigned int here too\n",
    "                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    memory_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))\n",
    "    return df\n",
    "\n",
    "데이터 = reduce_memory(데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54c5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoenv",
   "language": "python",
   "name": "geoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
