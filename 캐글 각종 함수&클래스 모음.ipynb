{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a5251f",
   "metadata": {},
   "source": [
    "# 1. LightGBM으로 CV하는 함수\n",
    "- Home Credit 대회 start-here-a-gentle-introduction 노트북"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c655d-cf44-4ecb-91b4-e989df40acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "def model(features, test_features, encoding = 'ohe', n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or \n",
    "            'le' for integer label encoding\n",
    "        n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics\n",
    "\n",
    "submission, fi, metrics = model(app_train, app_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973e38f",
   "metadata": {},
   "source": [
    "# Groupby 방법들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby 뜯어보는 방법\n",
    "gp = df.groupby('변수')\n",
    "\n",
    "ele_list = []\n",
    "for i, ele in gp:\n",
    "    ele_list.append(ele)\n",
    "\n",
    "gp_frame = DataFrame(columns=ele.columns)\n",
    "for ele in ele_list:\n",
    "    gp_frame = pd.concat([gp_frame, ele])\n",
    "    \n",
    "    \n",
    "    \n",
    "# 또다른 방법\n",
    "df['groupby변수'].value_counts()   # 여기서 갯수가 많은 값 중 하나를 골라서\n",
    "pay[pay['groupby변수'] == 값]\n",
    "pay[pay['groupby변수'] == 값][['groupby변수', 'agg변수']]  # 로 계산해서 뜯어볼 수 있음\n",
    "\n",
    "# 같은 내용 loc로 인덱싱\n",
    "cc.loc[(cc['SK_ID_PREV'] == 1079561), ['SK_ID_PREV', 'MONTHS_BALANCE']]\n",
    "\n",
    "\n",
    "# groupby 칼럼이 여러개일 때\n",
    "gp = df[df['groupby_1변수'] == '1변수값'][['groupby변수들']].sort_values(['groupby_나머지변수'])\n",
    "# reset_index를 해서 인덱스를 칼럼으로 변경\n",
    "gp = gp.reset_index()\n",
    "gp[gp['groupby_1변수'] == '1변수값']\n",
    "# loc로 index에 접근할 수도 있음\n",
    "gp.loc['1변수값']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "\n",
    "def do_mean(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_median(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].median().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_std(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_sum(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "group = ['ORGANIZATION_TYPE', 'NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'AGE_RANGE', 'CODE_GENDER']\n",
    "\n",
    "df = do_median(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_MEDIAN')\n",
    "df = do_std(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_STD')\n",
    "df = do_mean(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_MEAN')\n",
    "df = do_std(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_STD')\n",
    "df = do_mean(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_MEAN')\n",
    "df = do_std(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_STD')\n",
    "df = do_mean(df, group, 'AMT_CREDIT', 'GROUP_CREDIT_MEAN')\n",
    "df = do_mean(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_MEAN')\n",
    "df = do_std(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_STD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2cee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "BUREAU_AGG = {\n",
    "    'SK_ID_BUREAU': ['nunique'],\n",
    "    'DAYS_CREDIT': ['min', 'max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "    'AMT_ANNUITY': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean', 'sum'],\n",
    "    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n",
    "    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "    # Categorical\n",
    "    'STATUS_0': ['mean'],\n",
    "    'STATUS_1': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "    'STATUS_C': ['mean'],\n",
    "    'STATUS_X': ['mean'],\n",
    "    'CREDIT_ACTIVE_Active': ['mean'],\n",
    "    'CREDIT_ACTIVE_Closed': ['mean'],\n",
    "    'CREDIT_ACTIVE_Sold': ['mean'],\n",
    "    'CREDIT_TYPE_Consumer credit': ['mean'],\n",
    "    'CREDIT_TYPE_Credit card': ['mean'],\n",
    "    'CREDIT_TYPE_Car loan': ['mean'],\n",
    "    'CREDIT_TYPE_Mortgage': ['mean'],\n",
    "    'CREDIT_TYPE_Microloan': ['mean'],\n",
    "    # Group by loan duration features (months)\n",
    "    'LL_AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'LL_DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'LL_STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "# 각 feature마다 적용하고 싶은 함수를 각각 따로 딕셔너리로 지정해서 aggregation\n",
    "bureau.groupby('SK_ID_CURR').agg(BUREAU_AGG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homecredit 7등 솔루션\n",
    "\n",
    "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by='SK_ID_CURR'):\n",
    "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by=aggregate_by)\n",
    "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n",
    "\n",
    "def group(df_to_agg, prefix, aggregations, aggregate_by='SK_ID_CURR'):\n",
    "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper()) \n",
    "                               for e in agg_df.columns.tolist()])\n",
    "    return agg_df.reset_index()\n",
    "\n",
    "BUREAU_CLOSED_AGG = {\n",
    "    'DAYS_CREDIT': ['max', 'var'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'sum'],\n",
    "    'DAYS_CREDIT_UPDATE': ['max'],\n",
    "    'ENDDATE_DIF': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "for credit_type in ['Consumer credit', 'Credit card', 'Mortgage', 'Car loan', 'Microloan']:\n",
    "    type_df = bureau[bureau['CREDIT_TYPE_' + credit_type] == 1]\n",
    "    prefix = 'BUREAU_' + credit_type.split(' ')[0].upper() + '_'\n",
    "    agg_bureau = group_and_merge(type_df, agg_bureau, prefix, BUREAU_LOAN_TYPE_AGG)\n",
    "    del type_df; gc.collect()\n",
    "    \n",
    "for time_frame in [6, 12]:\n",
    "    prefix = \"BUREAU_LAST{}M_\".format(time_frame)\n",
    "    time_frame_df = bureau[bureau['DAYS_CREDIT'] >= -30*time_frame]\n",
    "    agg_bureau = group_and_merge(time_frame_df, agg_bureau, prefix, BUREAU_TIME_AGG)\n",
    "    del time_frame_df; gc.collect()\n",
    "    \n",
    "    \n",
    "agg_prev = group(prev, 'PREV_', {**PREVIOUS_AGG, **categorical_agg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homecredit 7등 솔루션\n",
    "\n",
    "sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "gp = sort_pos.groupby('SK_ID_PREV')\n",
    "df = pd.DataFrame()\n",
    "df['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00099a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homecredit 7등 솔루션\n",
    "# max값의 index를 가져오는 방법\n",
    "last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36dd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# homecredit 7등 솔루션\n",
    "\n",
    "gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n",
    "\n",
    "# 이외에 first(), last()등도 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7aec1a",
   "metadata": {},
   "source": [
    "# df.rename 방법들\n",
    "# columns 방법들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ede95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "\n",
    "gr.rename({'AMT_CREDIT_MAX_OVERDUE': 'BUREAU_LAST_LOAN_MAX_OVERDUE'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9eeab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "features = ['AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUM', \n",
    "            'AMT_CREDIT_SUM_DEBT', 'DEBT_PERCENTAGE', 'DEBT_CREDIT_DIFF', 'STATUS_0', 'STATUS_12345']\n",
    "agg_length.rename({feat: 'LL_' + feat for feat in features}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션, 칼럼이 2중으로 된 것 하나로 합치기\n",
    "agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper()) for e in agg_df.columns.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 칼럼명에 특수문자 제거\n",
    "# \" , : [ ] { } 등의 문자가 포함되어 있을 경우 에러 발생(lightgbm github에서)\n",
    "\n",
    "import re\n",
    "df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "# 혹은\n",
    "columns_del_special = [re.sub('[^A-Za-z0-9_]+', '_', x) for x in list(df[predictors].columns)]\n",
    "df[predictors].columns = pd.Index(columns_del_special)\n",
    "\n",
    "# 혹은 이렇게도 가능한데 \n",
    "aaaa = [x.replace('[', '_') for x in aaaa]\n",
    "aaaa = [x.replace(']', '_') for x in aaaa]\n",
    "aaaa = [x.replace('{', '_') for x in aaaa]\n",
    "aaaa = [x.replace('}', '_') for x in aaaa]\n",
    "aaaa = [x.replace(':', '_') for x in aaaa]\n",
    "aaaa = [x.replace('\"', '_') for x in aaaa]\n",
    "aaaa = [x.replace(',', '_') for x in aaaa]\n",
    "# 실제 df의 column들도 바꿔야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89e473",
   "metadata": {},
   "source": [
    "# df.isin 방법들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "pay[pay['SK_ID_PREV'].isin(active_df['SK_ID_PREV'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3b383",
   "metadata": {},
   "source": [
    "# df.reset_index() 방법들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412641f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "# df의 index를 column으로 만드는 용도\n",
    "active_pay_agg.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d06d6",
   "metadata": {},
   "source": [
    "# df.Apply() 방법들\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "pay['LATE_PAYMENT'] = pay['LATE_PAYMENT'].apply(lambda x: 1 if x > 0 else 0)\n",
    "(pay['LATE_PAYMENT'] > 0).astype(int)   # 둘다 같은 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06868c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "# 2개 이상의 칼럼에서 조건을 지정할 때 axis=1 지정\n",
    "df2.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0 and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32af167",
   "metadata": {},
   "source": [
    "# functools.partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99fccc",
   "metadata": {},
   "source": [
    "- 하나 이상의 인수가 이미 채워진 함수의 새 버전을 만들기 위해 사용된다.\n",
    "\n",
    "- 함수의 새 버전은 그 자체를 기술 하고 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fec666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(base, exponent):\n",
    "    return base ** exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404aac4",
   "metadata": {},
   "source": [
    "이제 정해진 지수 2와 3을 갖는 전용 사각형 및 큐브 함수를 원한다면 어떻게 될까?\n",
    "이때 우리는 다음과 같이 할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f16ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(base):\n",
    "    return power(base, 2)\n",
    "\n",
    "def cube(base):\n",
    "    return power(base, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e039c9",
   "metadata": {},
   "source": [
    "이 방법이 나름 효과적이지만 power() 함수의 변형을 15 개 또는 20 개 만들려면 어떻게 해야 할까? 1000개는? 그렇게 많은 반복적인 코드를 작성하는 것은 말할 필요도 없이 짜증나는 일이다. 이런 일을 해야할때 partials를 사용한다.\n",
    "\n",
    "사각형과 큐브 함수를 다시 작성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87946bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "square = partial(power, exponent=2)\n",
    "cube = partial(power, exponent=3)\n",
    "\n",
    "def test_partials():\n",
    "    assert square(2) == 4\n",
    "    assert cube(2) == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7acfc",
   "metadata": {},
   "source": [
    "이 부분함수에 대한 속성은 아래와 같이 기술 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab694df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_partial_docs():\n",
    "    assert square.keywords == {\"exponent\": 2}\n",
    "    assert square.func == power\n",
    "\n",
    "    assert cube.keywords == {\"exponent\": 3}\n",
    "    assert cube.func == power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8022d5e9",
   "metadata": {},
   "source": [
    "# Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbfd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "\n",
    "def chunk_groups(groupby_object, chunk_size):\n",
    "    n_groups = groupby_object.ngroups\n",
    "    group_chunk, index_chunk = [], []\n",
    "    for i, (index, df) in enumerate(groupby_object):\n",
    "        group_chunk.append(df)\n",
    "        index_chunk.append(index)\n",
    "        if (i + 1) % chunk_size == 0 or i + 1 == n_groups:\n",
    "            group_chunk_, index_chunk_ = group_chunk.copy(), index_chunk.copy()\n",
    "            group_chunk, index_chunk = [], []\n",
    "            yield index_chunk_, group_chunk_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0aba4",
   "metadata": {},
   "source": [
    "# 문자열 포매팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c23828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homecredit 7등 솔루션\n",
    "features = {}\n",
    "period = 12\n",
    "feature_name = 'DPD'\n",
    "prefix = '{}_TREND_'.format(period)\n",
    "gr = 데이터프레임\n",
    "\n",
    "def add_trend_feature(features, gr, feature_name, prefix):\n",
    "    y = gr[feature_name].values\n",
    "    try:\n",
    "        x = np.arange(0, len(y)).reshape(-1, 1)\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(x, y)\n",
    "        trend = lr.coef_[0]\n",
    "    except:\n",
    "        trend = np.nan\n",
    "    features['{}{}'.format(prefix, feature_name)] = trend\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683744b4",
   "metadata": {},
   "source": [
    "# df.memory_usage() \n",
    "- 메모리 사용량 줄이는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b4078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory(df):\n",
    "    \"\"\"Reduce memory usage of a dataframe by setting data types. \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Initial df memory usage is {:.2f} MB for {} columns'\n",
    "          .format(start_mem, len(df.columns)))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            cmin = df[col].min()\n",
    "            cmax = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # Can use unsigned int here too\n",
    "                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    memory_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52c910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoenv",
   "language": "python",
   "name": "geoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
